% !TeX spellcheck = en_GB
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                        %
%     Master thesis LaTeX template       %
%  compliant with the SZJK regulations   %
%                                        %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                        %
%  (c) Krzysztof Simiński, 2018-2023     %
%                                        %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                        %
% The latest version of the templates is %
% available at                           %
% github.com/ksiminski/polsl-aei-theses  %
%                                        %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%%
% This LaTeX project formats the final thesis
% with compliance to the SZJK regulations.
% Please to not change formatting (fonts, margins,
% bolds, italics, etc).
%
% You can compile the project in several ways.
%
% 1. pdfLaTeX compilation
%
% pdflatex main
% bibtex   main
% pdflatex main
% pdflatex main
%
%
% 2. XeLaTeX compilation
%
% Compilation with the XeLaTeX engine inserts Calibri font
% in the title page. Of course the font has to be installed.
%

% xelatex main
% bibtex  main
% xelatex main
% xelatex main
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% If you have any questions, remarks, just send me an email: %
%            krzysztof.siminski(at)polsl.pl               %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% We would like to improve the LaTeX templates
% of final theses. By answering the questions
% in the survey whose address your can find below
% you help us to do so. The survey is completely
% anonimous. Thank you!
%
% https://docs.google.com/forms/d/e/1FAIpQLScyllVxNKzKFHfILDfdbwC-jvT8YL0RSTFs-s27UGw9CKn-fQ/viewform?usp=sf_link
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                             %
% CUSTOMISATION OF THE THESIS                 %
%                                             %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Please customise your thesis with the macros below.

% TODO
%% author:
\newcommand{\FirstNameAuthor}{Klaudia}
\newcommand{\SurnameAuthor}{Janecka}
\newcommand{\IdAuthor}{$\langle$student id$\rangle$}  % (remove $\langle$ and $\rangle)

% coauthor:
%\newcommand{\FirstNameCoauthor}{First Names}  % If there is a coauthor, put the first names here.
%\newcommand{\SurnameCoauthor}{Surname}        % If there is a coauthor, put the surnames here.
%\newcommand{\IdCoauthor}{$\langle$student id$\rangle$} % If there is a coauthor, put the student id here (remove $\langle$ and $\rangle)
% If there is no coathor, leave the definitions empty like below. If a coauthor exists, comment the lines below.
\newcommand{\FirstNameCoauthor}{} % If there is only one author, leave the definitions empty.
\newcommand{\SurnameCoauthor}{}   % If there is only one author, leave the definitions empty.
\newcommand{\IdCoauthor}{}        % If there is only one author, leave the definitions empty.
%%%%%%%%%%

\newcommand{\Supervisor}{$\langle$title first name surname$\rangle$}  % supervisor (remove $\langle$ and $\rangle)
\newcommand{\Title}{Design and implementation of a microservice-oriented web application that collects data from external services.}
\newcommand{\TitleAlt}{Thesis title in Polish}
\newcommand{\Program}{Informatyka}
\newcommand{\Specialisation}{Informatics}
\newcommand{\Id}{$\langle$your student id$\rangle$}                   % remove \langle and \rangle in final version
\newcommand{\Departament}{$\langle$departament name$\rangle$}         % remove \langle and \rangle in final version
\newcommand{\Surname}{Janecka}
\newcommand{\Firstnames}{Klaudia}

% If you have a consultant for your thesis, put their name below ...
%\newcommand{\Consultant}{$\langle$title first name surname$\rangle$}  %  (remove $\langle$ and $\rangle)
% ... else leave the braces empty:
\newcommand{\Consultant}{} % no consultant

% end of thesis customisation
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                             %
% END OF CUSTOMISATION                        %
%                                             %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                             %
%   PLEASE DO NOT MODIFY THE SETTINGS BELOW!  %
%                                             %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass[12pt]{report}
\usepackage[utf8]{inputenc}                                      
\usepackage[T1]{fontenc}  
\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage[polish,british]{babel} 
\usepackage{indentfirst}
\usepackage{xurl}
\usepackage{xstring}
\usepackage{ifthen}
\usepackage{enumitem}
\usepackage{tabularx} 
\usepackage{nameref}
\usepackage{ifxetex}
\usepackage{multirow}
\ifxetex
	\usepackage{fontspec}
	\defaultfontfeatures{Mapping=tex—text} % to support TeX conventions like ``——-''
	\usepackage{xunicode} % Unicode support for LaTeX character names (accents, European chars, etc)
	\usepackage{xltxtra} % Extra customizations for XeLaTeX
\else
	\usepackage{lmodern}
\fi



\usepackage[margin=2.5cm]{geometry}
\usepackage{graphicx} 
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{mathtools}
\usepackage{geometry}
\usepackage{subcaption}   % subfigures
\usepackage[page]{appendix} % toc,
\usepackage{caption}
\usepackage{adjustbox}
\setcounter{tocdepth}{5}
\setcounter{secnumdepth}{5}
\setcounter{secnumdepth}{6}
\usepackage{titlesec}
% Define subsubsubsection command

\usepackage{csquotes}

\usepackage[backend=biber]{biblatex}
\addbibresource{mybibliography.bib} 

\usepackage{ifmtarg}   % empty commands  

\usepackage{setspace}
\onehalfspacing


\frenchspacing



%%%% TODO LIST GENERATOR %%%%%%%%%

\usepackage{color}
\definecolor{brickred}      {cmyk}{0   , 0.89, 0.94, 0.28}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\makeatletter \newcommand \kslistofremarks{\section*{Remarks} \@starttoc{rks}}
  \newcommand\l@uwagas[2]
    {\par\noindent \textbf{#2:} %\parbox{10cm}
{#1}\par} \makeatother


\newcommand{\ksremark}[1]{%
{%\marginpar{\textdbend}
{\color{brickred}{[#1]}}}%
\addcontentsline{rks}{uwagas}{\protect{#1}}%
}










%%%%%%%%%%%%%% END OF TODO LIST GENERATOR %%%%%%%%%%%  

\newcommand{\printCoauthor}{%		
    \StrLen{\FirstNameCoauthor}[\FNCoALen]
    \ifthenelse{\FNCoALen > 0}%
    {%
		{\large\bfseries\Coauthor\par}
	
		{\normalsize\bfseries \LeftId: \IdCoauthor\par}
    }%
    {}
} 

%%%%%%%%%%%%%%%%%%%%%
\newcommand{\autor}{%		
    \StrLen{\FirstNameCoauthor}[\FNCoALenXX]
    \ifthenelse{\FNCoALenXX > 0}%
    {\FirstNameAuthor\ \SurnameAuthor, \FirstNameCoauthor\ \SurnameCoauthor}%
	{\FirstNameAuthor\ \SurnameAuthor}%
}
%%%%%%%%%%%%%%%%%%%%%

\StrLen{\FirstNameCoauthor}[\FNCoALen]
\ifthenelse{\FNCoALen > 0}%
{%
\author{\FirstNameAuthor\ \SurnameAuthor, \FirstNameCoauthor\ \SurnameCoauthor}
}%
{%
\author{\FirstNameAuthor\ \SurnameAuthor}
}%

%%%%%%%%%%%% FANCY HEADERS %%%%%%%%%%%%%%%
% no capitalisation of headers
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[LO]{\nouppercase{\it\rightmark}}
\fancyhead[RE]{\nouppercase{\it\leftmark}}
\fancyhead[LE,RO]{\it\thepage}

\setlength{\headheight}{14.49998pt}
\addtolength{\topmargin}{-2.49998pt}

\fancypagestyle{onlyPageNumbers}{%
   \fancyhf{} 
   \fancyhead[LE,RO]{\it\thepage}
}

\fancypagestyle{noNumbers}{%
   \fancyhf{} 
   \fancyhead[LE,RO]{}
}


\fancypagestyle{PageNumbersChapterTitles}{%
   \fancyhf{} 
   \fancyhead[LO]{\nouppercase{\Firstnames\ \Surname}}
   \fancyhead[RE]{\nouppercase{\leftmark}} 
   \fancyfoot[CE, CO]{\thepage}
}
 


%%%%%%%%%%%%%%%%%%%%%%%%%%%








\newcounter{pagesWithoutNumbers}

%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\newcommand{\printOpiekun}[1]{%		

    \StrLen{\Consultant}[\mystringlen]
    \ifthenelse{\mystringlen > 0}%
    {%
       {\large{\bfseries CONSULTANT}\par}
       
       {\large{\bfseries \Consultant}\par}
    }%
    {}
} 
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 
% Please do not modify the lines below!
\newcommand{\Author}{\FirstNameAuthor\ \MakeUppercase{\SurnameAuthor}} 
\newcommand{\Coauthor}{\FirstNameCoauthor\ \MakeUppercase{\SurnameCoauthor}}
\newcommand{\Type}{MASTER THESIS}
\newcommand{\Faculty}{Faculty of Automatic Control, Electronics and Computer Science}
\newcommand{\Polsl}{Silesian University of Technology}
\newcommand{\Logo}{politechnika_sl_logo_bw_pion_en.pdf}
\newcommand{\LeftId}{Student identification number}
\newcommand{\LeftProgram}{Programme}
\newcommand{\LeftSpecialisation}{Specialisation}
\newcommand{\LeftSUPERVISOR}{SUPERVISOR}
\newcommand{\LeftDEPARTMENT}{DEPARTMENT}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                             %
% END OF SETTINGS                             %
%                                             %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                             %
% MY PACKAGES, SETTINGS ETC.                  %
%                                             %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Put your packages, macros, setting here.


 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% listings
% packages: listings or minted
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 

% package listings
\usepackage{listings}
\lstset{%
morekeywords={var,get,set},% add the keyword you need
language=[Sharp]C% C, Matlab, Python, SQL, TeX, XML, bash, ... – vide https://www.ctan.org/pkg/listings
commentstyle=\textit,%
identifierstyle=\textsf,%
keywordstyle=\sffamily\bfseries, %\texttt, %
%captionpos=b,%
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
tabsize=3,%
frame=lines,%
numbers=left,%
numberstyle=\tiny,%
numbersep=5pt,%
breaklines=true,%
breakpages=false,%
escapeinside={@*}{*@},%
}

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 
% package minted
% \usepackage{minted}

% This package requires a special command line option in compilation
% pdflatex -shell-escape main.tex
% xelatex  -shell-escape main.tex

%\usepackage[chapter]{minted} % [section]
%%\usemintedstyle{bw}   % black and white codes
%
%\setminted % https://ctan.org/pkg/minted
%{
%%fontsize=\normalsize,%\footnotesize,
%%captionpos=b,%
%tabsize=3,%
%frame=lines,%
%framesep=2mm,
%numbers=left,%
%numbersep=5pt,%
%breaklines=true,%
%escapeinside=@@,%
%}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                             %
% END OF MY PACKAGES, SETTINGS ETC.           %
%                                             %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
%\kslistofremarks

\frontmatter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                             %
%    PLEASE DO NOT MODIFY THE TITLE PAGE!     %
%                                             %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%  TITLE PAGE %%%%%%%%%%%%%%%%%%%
\pagestyle{empty}
{
	\newgeometry{top=1.5cm,%
		bottom=2.5cm,%
		left=3cm,
		right=2.5cm}
	 
	\ifxetex 
	\begingroup
	\setsansfont{Calibri}
		   
	\fi 
	\sffamily
	\begin{center}
		\includegraphics[width=50mm]{\Logo}
			 
			
		{\Large\bfseries\Type\par}
			
		\vfill  \vfill  
					 
		{\large\Title\par}
			
		\vfill  
				
		{\large\bfseries\Author\par}
			
		{\normalsize\bfseries \LeftId: \IdAuthor}
		
		\printCoauthor
			
		\vfill  		
		 
		{\large{\bfseries \LeftProgram:} \Program\par} 
			
		{\large{\bfseries \LeftSpecialisation:} \Specialisation\par} 
			 		
		\vfill  \vfill 	\vfill 	\vfill 	\vfill 	\vfill 	\vfill  
			 
		{\large{\bfseries \LeftSUPERVISOR}\par}
			
		{\large{\bfseries \Supervisor}\par}
						
		{\large{\bfseries \LeftDEPARTMENT\ \Departament} \par}
				
		{\large{\bfseries \Faculty}\par}
				
		\vfill  \vfill  
		
		    	
		\printOpiekun{\Consultant}
		    
		\vfill  \vfill  
				
		{\large\bfseries  Gliwice \the\year}
		
	\end{center}	
	\ifxetex 
	\endgroup
	\fi
	\restoregeometry
}
  


\cleardoublepage

\rmfamily\normalfont
\pagestyle{empty}


%%% Let's start the thesis %%%%

% TODO
\subsubsection*{Thesis title}  
\Title

\subsubsection*{Abstract} 
(Thesis abstract – to be copied into an appropriate field during an electronic submission – in English.)
The investigations covered in this papers are related to the Microservices architecture. The main aim is to investigate the communication aspects within microservices architecture and their impact on the system performance, scalability and it's overall efficiency.
The knowledge about the inter-services communication is crucial to build robust distributed systems. 
The study delves into various communication patterns and technologies commonly used in the microservices architecture, providing insights about its limitations, advantages and impact on the whole system behaviour. 
 
\subsubsection*{Key words}  


\subsubsection*{Tytuł pracy}
\begin{otherlanguage}{polish}
	\TitleAlt
\end{otherlanguage}

\subsubsection*{Streszczenie} 
\begin{otherlanguage}{polish}
	(Streszczenie pracy – odpowiednie pole w systemie APD powinno zawierać kopię tego streszczenia.)
\end{otherlanguage}

\subsubsection*{Słowa kluczowe} 
\begin{otherlanguage}{polish}
	
\end{otherlanguage}




%%%%%%%%%%%%%%%%%% Table of contents %%%%%%%%%%%%%%%%%%%%%%
% Add \thispagestyle{empty} to the toc file (main.toc), because \pagestyle{empty} doesn't work if the TOC has multiple pages
\addtocontents{toc}{\protect\thispagestyle{empty}}
\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\setcounter{pagesWithoutNumbers}{\value{page}}
\mainmatter
\pagestyle{empty}

\cleardoublepage
\pagestyle{PageNumbersChapterTitles}

%%%%%%%%%%%%%% body of the thesis %%%%%%%%%%%%%%%%%

% TODO
\chapter{Introduction}
The following chapter will cover the introduction to the problem domain. It will define the scope of work, present a brief description of the developed system, and outline the conducted research.

\section{Introduction to the problem domain}
Nowadays, IT solutions are commonly the core of maintenance and organization of many products and companies. The system for any enterprise with a notable size is  very complex and composed of a  few independent solutions. The growth of the system commonly relates to the new system functionalities, connection to resources provided by new business partners or new technological solutions, such as cloud services, which have recently been a trend in software development. Due to the connection between them, such as infrastructure is for example the so called ``Spaghetti architecture". [figure 1.1]  illustrates an example of an inefficient and hard to manage IT solution.

\begin{figure}
	\centering
	\includegraphics[width=0.5\textwidth]{spaghetti.jpg}
	\caption{Spaghetti architecture diagram}
	\label{fig1}
\end{figure}

Referring to this problem, many architectural patterns have been created over the years. Microservices are an example of an approach of software architecture which addresses many modern application challenges ~\cite{microserviceschallanges:1} . Microservices offer several advantages including scalability and flexibility in managing complex applications. The most important point of designing microservices architecture is a good selection of technology and a method of communication between individual subsystems. The choice of the pattern is sophisticated, because it requires cross-sectional knowledge of a wide range of technologies available on the market. The technology should meet the requirements of the individual system and simultaneously is supposed to  be optimal     in relation to the costs.

\section{Contribution and objective of the thesis}
The main purpose of the thesis is to provide the solution to the choice of a proper type of communication in the process of creating a  (complex distributed system in microservices architecture.
The goals of the thesis are as follows:
\begin{itemize}
	\item To study and analyze existing communication technologies, 
	\item To identify the challenges of collaboration between microservices,
	\item To experimentally verify selected types of communication and various data packages,
	\item To identify the configuration properties required to establish  the best performance communication between microservices by means of using selected technology,
	\item To study the best choose of technology depending on the demand of the system, => TODO
	\item To examine the cost and difficulties in implementing selected technology pattern.
\end{itemize}
For the purpose of conducting a  research for the  master’s thesis, a system based on the microservices architecture was developed, along with the creation of datasets.
The system is composed of multiple applications. It creates the space to perform experiments with multiple communication patterns. There is one publisher application which sends the data to multiple separate client’s applications. Each application stands for the representation of each communication pattern.
Datasets were designed in various forms to perform experiments for a few different purposes of communication between subsystems. Over the study, metrics have been collected at multiple stages [tutaj moze wiecej dopisze jak skoncze badania].
The study ends with a simulation emulating the real-word scenario of data transmission. The data is sent to the robotics simulator  that simulates  environment simultaneously  impacting the data transmission to the robot. This action triggers the robot’s motion. It becomes possible to gain the insights about how the type of communication influences  the trajectory and  at the same time evaluate the  efficiency  of the robot’s motion. The findings of this study have the potential to contribute to the development of more efficient and optimized communication strategies within microservices-based applications.

\section{Overview of the literature}
In this section, a detailed state of the art review is presented, and this analysis was restricted to the papers published between 2015 and 2023. The  information about the communication types and data serialization techniques was  collected from various sources such as official documents, scientific theses, blogs and video conferences. The collaboration between microservices is currently under the constant development and research. New technology solutions, such as cloud architecutres enables to increase the performance and efficiency of the communication.  Most of literatures provide a brief comparison between  the two selected  technologies. However  these  kinds  of research were also very helpful. They allow analyzing a specific use case of the  chosen patterns. 
The  following articles which were found  during the investigation, and they are the closest to the topic of  master’s thesis.

In \cite{performanceEvaluation:2} Fernandes et al. provided the survey of performance in REST API in competition to Advanced Message Queuing Protocol in mobile development environment. In the article, the authors present a description of these technologies, with a focus on the introduction of asynchronous communication as a promising solution to data transfer using massive quantities of data. For performance studies RabbitMQ supporting the AMPQ Protocol is taken into consideration. To provide a result, the authors conduct multiple experiments in created Java back-end system using large amounts of data. In the first experiment, the authors focused on examining the interaction between a user application and RabbitMQ, a message broker, using the AMQP protocol. Throughout a thirty minute experiment, the user application continuously sent messages to RabbitMQ. However, a notable aspect of this experiment was the absence of any active consumer (back-end service) connected to RabbitMQ during the message transmission. This design allowed the authors to observe RabbitMQ's behavior when handling incoming messages without immediate recipients. Moving on to the second experiment, the user application's objective was to send messages to a Web service server for a continuous period of 30 minutes, utilizing the HTTP protocol for communication. Subsequently, the Web service, upon receiving these messages, established communication with a database using the Java Database Connectivity driver to store the incoming messages. Through this setup, the authors sought to evaluate the end-to-end message flow, analyzing the efficiency and reliability of the communication and data storage process between the user application, Web service, and the database. In the third experiment, the user application once again sent messages to RabbitMQ using the AMQP protocol, as described in the first experiment. However, this time, the authors introduced an active consumer (back-end service) connected RabbitMQ. The consumer's role involved reading the incoming messages from RabbitMQ, processing them, and using the Java Database Connectivity driver driver to connect to the database and store the processed messages. This experimental configuration allowed the authors to assess the overall performance and effectiveness of the message processing pipeline, particularly in a real-time scenario with an active consumer. The authors found out  that the application which is  supposed to exchange an large amount of data should use RabbitMQ. They confirmed that RabbitMQ  sent the larger number of messages per second and consumed less mobile device resources

In \cite{multitenant:3} thesis author analyzes the multitenant system in microservices architecture which is based on the events. A significant key is that one instance called tenant manager serves multiple  tenants . It gains more  economical benefits than by  creating tenant manager instance for each customer’s  application. The use of microservices is significant to solve the problem because it helps in keeping all the  modules independent, consequently allowing tenants to choose  their own technology stack and in continous-delivery to perform some customization quicky   without  affecting other tenant ‘s system. It is a perfect solution  to  a  tenant customization to meet  a single customer’s  needs and a feature requested by one organisation without  affecting  any of the other organization. The system presented in the paper focuses on the tenant-islotation and the reduced numer of application programming interface (API) calls. Microservices are communicating with each other using RabbitMQ and Azure Service Bus with isolated topics for each tenant. Author provides complex analyzys of benefits of each event bus, putting focus on the security and the ease of setting up the environment.
 
In \cite{kamppuri2014message} Kamppuri and Tsuri presented a comprehensive study titled 'MESSAGE BROKERS AND RABBITMQ IN ACTION,' which was submitted as a Bachelor’s Thesis in the Degree Programme of Media Engineering. The thesis was supervised by Manninen, Pasi and was assigned by Paytrail Oyj. Published on May 15, 2014, the work comprises 40 pages and is written in English. The permission for web publication was granted.
The objectives of the bachelor’s thesis were twofold: firstly, to delve into the concept of messaging and messaging systems within the realm of Information Sciences, and secondly, to assess the applicability of RabbitMQ as a potential replacement for existing systems at Paytrail Oyj.
In the practical section, the thesis investigates the feasibility of substituting a web API with a message queue solution. This exploration encompasses elucidating the benefits, implementation stages, and command-line examples. Furthermore, the practical segment extrapolates scenarios wherein message queues could prove advantageous for Paytrail Oyj.The outcome of the thesis manifests as a comprehensive theoretical resource aiding the selection of an appropriate message broker solution. Moreover, the thesis offers detailed guidance on integrating RabbitMQ into existing systems, along with innovative suggestions for other potential applications of message queues. These application ideas are designed to facilitate future development endeavors by the client.

In \cite{riera2022asynchronous}, Martí Riera presented a master's thesis titled "Asynchronous event-reactive system in a microservices-based architecture." The work explores the implementation and deployment of a reactive system for event handling within an asynchronous software architecture. The project addresses various needs related to Data Tracking, Compliance, and Customer Relationship Management (CRM) through event-triggered actions. To address the challenge of recurrent code changes and deployment costs, a dashboard-based configuration approach is adopted. The system involves entities such as the server receiving events (Event Handling Agency) and a Strapi dashboard, both deployed as microservices on Heroku. The technologies employed include NodeJS, MongoDB, RabbitMQ, Expo, and SendGrid.

In the thesis by Raje \cite{raje2019performance}, the concept of message queues is explored as a means of enabling communication between applications through sequential message or event handling. This asynchronous communication is pivotal for decoupling and scalability within distributed systems. The study focuses on an intricate performance analysis and comparison of three prominent open-source message brokers: Apache ActiveMQ, RabbitMQ, and Apache Kafka. Real-world application scenarios are replicated through end-to-end message queuing models, followed by rigorous benchmarking tests on producers, consumers, and brokers. The evaluation encompasses key performance metrics, including throughput, latency, and transaction duration. Results indicate that Apache Kafka, originally designed as a message queue but evolved into a robust streaming platform, excels in performance over RabbitMQ and Apache ActiveMQ across various aspects. Notably, all three brokers demonstrate consistent performance trends with larger message sizes, underscoring the importance of considering small message sizes for challenging scenarios. This thorough analysis provides valuable insights into the inherent performance capacities of message queuing brokers.

In the work conducted by Akilli \cite{akilli2018analysis}, the analysis of transformation capabilities between communication types of cloud application components is investigated. Cloud application components and their interconnections can be modeled through standards like the Topology and Orchestration Specification for Cloud Applications (TOSCA) language to streamline deployment, management, and portability efforts. However, changes in internal or external conditions necessitate updates to the model. In certain scenarios, components initially situated on the same host might get distributed across different hosts. While existing studies on such component distribution focus on compatibility with new hosting environments, the essential communication adaptations are often overlooked.

Following the separation of topologies, the communication channels previously supported by the original environment might not be viable in the new context. Consequently, communication mechanisms among the components must be transformed and updated. If a successful transformation is not feasible, the separation process may need to be reconsidered. This thesis delves into the analysis of transformation capabilities between various communication types of cloud application components. Communication protocols are classified based on their characteristics, and the potential for transitioning between these communication types, along with the associated challenges and limitations, is thoroughly examined.

Furthermore, a prototype is developed as part of the research, which identifies the need for transformation when the hosting environment undergoes changes. This prototype also offers decision support for redistributing the components, taking into account the required communication transformations. The study contributes insights into the critical aspects of communication evolution during component separation and reallocation, shedding light on the intricate intricacies involved in such processes

In his study titled 'Azure Service Bus: en kravstudie,' Larsson \cite{larsson2017azure} investigates the compatibility of Microsoft Azure Service Bus with the needs of Sogeti and its clients during the shift to a distributed and service-oriented system. A Proof-of-Concept model is developed for simulations and evaluations. Results indicate that Azure Service Bus meets the defined requirements and ensures secure communication. The study concludes with an analysis of the findings, proposing further research possibilities and recommendations for the client's future steps.

In their work 'Developing Cloud-Native Solutions with Microsoft Azure and .NET' \cite{satapathi2023build}, Satapathi and Mishra delve into the construction of decoupled solutions using Azure Service Bus, Microsoft Azure's enterprise-grade messaging solution. This chapter explores the significance of message brokers in facilitating asynchronous inter-application or service communication within resilient and scalable cloud-native solutions. It highlights Azure Service Bus among various messaging systems available, emphasizing its role in building robust cloud-native architectures.

After the completion of literature review, it is confirmed that this topic  poses a  very common problem nowadays   and the choice of an appropriate connection type and its configuration when designing web applications becomes quite a big challenge.

\section{Short summary of chapters}

TODO IN THE LATE STATE


\chapter{Overview of terminology and technology evolution}

This chapter covers the brief description of software architectures and patterns strictly related with the problem domain. It includes a comparison of  base architectures in relation to the microservices. This chapter also provides an comprehensive analysis of the evolution of Microservices architecture. It presents its origins, emphasizing the core motivation aspects leading to its creation. In this chapter the history context will be examined. It is worth mentioning that  the industrial software needs  resulted in  the development of the technology that shaped the emergence of microservices architecture as a prominent model in software engineering for complex systems.

\section{Architecture patterns}
\subsection{Monolith architecture}

To analyse the microservices it is worth mentioning the monolith architecture, which is  a traditional model of the software program. Monolith is built  as a single unit, all the code is deployed as an single unit. It is self-contained and independent of other applications. It can be composed of a single module or multiple modules inside a single process. Under one monolith application multiple developers teams could be working,  on different segments of the application. The disadvantage of monolith is the difficulty in making changes. Due to this inconvenience multiple problems with blocking each other by waiting for a change of the same piece of the code could occur. Any change, even a small one, involves updating the entire stack and deploying the updated version, which is very time consuming when the system is large \cite{monolith:4}.
The communication between modules in the monolith architecture is performed using a local method invocation. The intermodule communication could be a problematic solution. This pattern is direct and synchronous, so if the called module is not available within the system , the caller module will be impacted. The system could be blocked for the indefinite time or has to wait till the system returns the time-out errors, leading to report of the system issue. It becomes particularly significant for example when the calling and called methods are responsible for generating and managing data respectively. If the function responsible for data analysis encounters parsing error, also the subsystem responsible for generating data would stop. This underscores the critical importance of establishing a robust error handling mechanism and ensuring that all business logic relies on a system capable of perfect handling errors and appropriately responding to failures.
However monolith has also some advantages:
\begin{itemize}
	\item All the code located in one place is easier to debug,
	\item One executable file is easier to deploy than multiple files from multiple subsystems, 
	\item End-to-end testing can be performed faster than in the distributed systems.
\end{itemize}
\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\textwidth]{monolith.png}
	\caption{Monolith diagram}
	\label{fig1}
\end{figure}
 
\subsection{Service-oriented architecture}
The heavyweight nature of monolith architecture has slowly been decomposed decomposed into modules. This process has led to the formation of a new architecture known as service-oriented architecture (SOA).
Modules communicate using the Enterprise Service Bus. Software-oriented architecture manages and coordinates the services delivered through this layer. For the whole system Enterprise Service Bus is single point of failure and if any damage or delay occur the system the entire system could be affected and overwhelmed. Modules communicate using the Enterprise Service Bus. Software-oriented architecture manages and coordinates the services delivered through this layer. 
Modules perform  various type of operations and transactions. There is no strict division into individual role for a particular module. The whole system uses a single dedicated database engine. The application under consideration is comprised of four different types of services:

\begin{description}
	\item \textbf{Functional services}\\
	The services which are responsible for the businesses operations for the application. It encapsulates the business logic and provides functionality that can  be accessed by other components within the system.
	\item \textbf{Enterprise services}\\
	The services which implement the functionality required by the system. It integrates multiple functional services to deliver end-to-end functionality. The service which performs coordination of various business processes across different subsystems or applications, could be given as an example of enterprise service.
	\item \textbf{Application services}\\
	The services which are responsible for development and deployment. They provide the set of tools such as development frameworks, application deployment platforms, and runtime environments which could be used to perform build, deploy and manage the application within  the environment.
	\item \textbf{Infrastructure services}\\
	The services which are related to the underlying technical infrastructure of a system.  These services could be responsible for the authentication, security or data management. They could also handle the communication protocols.  	
\end{description}
The service-oriented architecture is a precursor of microservices architecture. How ever these architectures are neither comparable and nor compatible. There are a few significant discrepancies  between them. The Service-oriented architecture differs in the scope. In Service-oriented architecture modules are shared and reused enterprise-wide. For example the scope of the application may include the entire department in the company or  a complex subsystem, which could subsequently consist of full user’s  authorization  and the business logic (e.g. administrator module for user’s management).

\begin{figure}[!]
	\centering
	\includegraphics[width=0.6\textwidth]{soa.png}
	\caption{Service-oriented architecture diagram}
	\label{fig1}
\end{figure}
 
\subsection{Microservices architecture}
Service-oriented architecture was a precursor  to  microservices, laying the foundations  for the development of the  ideas related to the modularization of services and applications. Microservices, although inspired by SOA architecture, have extended the concept of modularity. Microservices not only applications in order to optimize the amount of functions and data, but also segregate operations in terms of business logic basing on the Domain Driven Design \cite{dddmicroservices:4}. Each microservice  has both, a  single responsibility for the system and a  separate database engine. Microservices are discussed in more detail in the next chapers.
 
\begin{figure}[h]
	\centering
	\includegraphics[width=0.6\textwidth]{microservices.png}
	\caption{Microservices diagram}
	\label{fig1}
\end{figure}
\subsubsection{Domain Driven Design}
Domain driven Design is an approach to the software development which concentrates on modelling software architecture to match a domain, which in context of software development reefers to the business. It leads to create the domain models, software abstractions encapsulating complex business logic, to compeer as much as is possible the business reality and code.  The main goal of Domain Driven Development is to bring the technical and and business aspects of the design process together \cite{Rosiek2021:12}.
DDD introduces the idea of strategic design, where the entire system is divided into bounded contexts, each encapsulating a specific domain and its associated futures. Bounded contexts provide a clear separation between different aspects of the system, enabling independent development and maintenance. 
At the heart of Domain-Driven Design is the concept of creating a ubiquitous language, a common vocabulary that is understood both by the technical team and the business stakeholders. This shared language acts as a bridge, fostering a deep understanding of the domain's intricacies across all parties involved. It ensures that technical discussions are carried out using terms that resonate with business experts, enhancing clarity and minimizing misunderstandings.


\chapter{Microservices architecture}
The term ``Microservice" defines the small application which has a single responsibility to the whole system. It has only one reason for changing or being replaced.
Adrian Cockcroft, a famous technologist and strategist, defines a microservices architecture as a service-oriented architecture consisting of loosely coupled components that operate within well-defined boundaries \cite{Adrian:5}. Loosely coupled means that services can be updated independently, without requiring changes to other services. If you have a collection of small, specialised services that still need to be updated simultaneously, they cannot be considered as microservices. There is a lack of loose coupling principle between them \cite{modelingwithddd:6}

The idea of the microservices is related to the demand of scaling along different axes independently. This is a big trend in modern software architecture so studying  it in the  perspective of communication and data flow is extremely important and even indispensable.

\section{Evolution of software architecture}

The history of microservices can be traced back to the 1990s, when the first ideas and concepts of distributed software architecture emerged. However, the term ’microservices’ has only become popular in the last decade, and their development was linked to advances in technology and changes in approaches to application development \cite{DeGiacomo2021:7}.

An Early work on microservices focused on finding better ways to create service-based software. One important step was the introduction of Service-Oriented Architecture (SOA) as an approach to creating modular and loosely coupled services. Service-Oriented Architecture has made it possible to create applications consisting of independent components that can be easily scaled and developed.
In 2011, the term ``microservices’ was introduced by Martin Fowler and James Lewis in their article \cite{fowler2014microservices:8}.
 
\subsection{Netflix as a microservices pioneer}
Netflix, Inc.  played an important role in developing and promoting microservices architecture. Netflix, Inc. was one of the pioneers in using microservices as the main approach to building its streaming platform \cite{8804433:9}. Netflix, Inc. started out as a small rental company. It offered an online DVD subscriptions through the Internet. Movies were mailed to the customers in the form of DVD’s with prepaid return envelopes. Nowadays Netflix, Inc. is huge streaming platform whose content is produced in houses (Netflix Originals) or produced by other companies which Netflix  has distribution right to. With the huge shift in product delivery from postal services to online services, Netflix, Inc. needed more and more resources on the Internet and powerful software, due to the increasing demand. Consequently, the company made a decision to move its IT infrastructure from private data centres to a public cloud and replace its monolithic architecture with a microservices architecture. However, at that time, the term ``microservices" was not widely known, and the structure itself was not well-established.
Netflix, Inc. emerged as one of the first prominent companies to successfully introduce the transition from  a monolith to a cloud-based microservices architecture. This achievement was recognized in 2015 when it received the JAX Special Jury award. Nowadays , Netflix, Inc. manages and supports  various parts of its platform through over a thousand microservices.

\section{Benefits and Challenges of Microservices}

Microservices architecture numerous benefits, nevertheless due to its greater complexity than legacy systems it also meets some challenges:
\begin{itemize}
	\item As the number of subservices grows so does the complexity of managing and maintaining the data consistency. The first problem is encountered at the beginning of creating system. Software architect has to find appropriate boundaries of the a subsystem. The whole business requirements have to be broken down into specific domains and the microservices that are sized adequately are supposed to be created. Unfortunately the whole process is extremely difficult to achieve.
	\item There is also difficulty in testing the whole system. In addition to testing individual services independently, the service integration and interdependencies should also be considered during creating a end-to-end test plan.
	\item Microservices standards allow subsystems to make subsystems using different technologies. It can cause many problems, starting from integrating microservices to use the same libraries for common operations and  finishing with transition to the maintenance mode. The advantages acquired from technical diversity may quickly get outweighed by increased maintenance costs,  generated by the necessity to arrange a  maintenance team, composed of developers who are knowledgeable about whole the technology stack. They have to be aware of the entire stack in order not to make  any breaking change in the functionality that other services depend on.
\end{itemize}
However, most of the challenges related to the microservices are around the necessity to share the business data and information between subsystems. All of that ought to be solved  with a good communication implementation, which will be discussed in the next chapter.

\section{Cloud and contenerization}
At the beginning  of  this section, one can find   brief descriptions of concepts followed by detailed analysis  further in the section subject.
\subsection{Containerization }
Containerization is a method of virtualization that operates at the application level. It allows for the creation of multiple isolated instances within a single operating system kernel. These instances are known as containers, which serve as self-contained units that pack an application’s code, runtime, system tools, libraries, and configurations into a single entity. Containers visualize CPU, memory, storage, and network resources at the operating system level, creating a segregated environment where developers can interact with the OS independently from other applications and environments. Containers share a common kernel, which is installed on the underlying hardware.

A container images serve a fundamental building block. It is a file that contains all the necessary components to run an application within a container. The image includes the application’s code, dependencies, libraries, system tools, and configurations. Images construction ,and creation is based on the configuration files, where there are specified steps required to build an image in the container instance.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.7\textwidth]{container.png}
	\caption{Container vs Casual}
	\label{fig1}
\end{figure}
\subsubsection{Docker}
Docker is a commonly used containerization platform. Currently is the most widely adopted open-source container format. It provides an additional layer of abstraction and automation for the container management. It simplifies the process of creating, deploying, and running applications within containers. Docker offers the ``docker compose" tool helping  to configure and run applications made up of multiple containers. For the docker users it provides an ecosystem of pre-build container images available through the internet on the Docker Hub website. It provides developers with the convenient way of reuse the existing solutions again.

\subsection{Orchestration}
The process of managing and arranging containers to support applications is known as a container orchestration. It is fully managed by container orchestration tools. These tools provide the necessary mechanisms to deploy, scale, manage, and monitor containerized applications. These tools help automate tasks such as container deployment, load balancing, service discovery, scaling, and fault tolerance, making it easier to manage complex containerized environments efficiently. Some widely used open-source container orchestration tools include Kubernetes, Docker Swarm, and LXC.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.7\textwidth]{orche.png}
	\caption{Container orchestrator}
	\label{fig1}
\end{figure}
\subsubsection{Kubernetess}
Kubernetes (known as K8S) is an open-source platform designed to automate the deployment, scaling, and management of containerized applications Kubernetess organizes containers that form an application into cohesive units, providing the easy way to manage complex system. Kubernetes is based on a  Google’s  fifteen year  extensive experience  in running  production workloads having  incorporated valuable insights and industry best practices gathered from the community.
\subsection{Continuous delivery}
Continuous Delivery is a software concept that enables the demand deployment of a software to any native environment. It enhances the delivery of software life circle to be automated. It leverages techniques like Continuous Integration and Continuous Deployment. 
\subsection{Microservices in cloud computing}
In recent years cloud computing gained the interest and trust of many researches and industries. This is a growing paradigm which had a big impact on the microsevices implementation. Currently on the market there are many cloud services providers such as Microsoft Azure, Amazon Web Services (AWS), Google Cloud Platform (GCP), IBM Cloud, and Kamatera. The usage of containers in the cloud makes microservices development and deployment more agile \cite{vayghan2019kubernetes:10}. Microservices are abstracted,  which means that they can be run on any operating system located in the public cloud, on premises or in the virtual hypervisor. They can be also migrated back from a cloud to the on-premises and back. Containers are a perfect solution for the deployment of microservices. They can be launched in a second, so redeployment of any failure or migration can be done rapidly. It helps to scale quickly to meet current demands. The entire microservice application can be deployed as a cluster using a orchestrator.

In the security aspect, using the same host kernel for different containers makes it more possible for attackers to gain  an unauthorized access to containers \cite{https://doi.org/10.1002/cpe.4436:11}. The containers which rely on the underlying operating system’s kernel provide the resources isolation. If an attacker manages to exploit the vulnerability in the shared kernel, they may gain an unauthorized access to the host system and potentially compromise all containers running on that host. It is called the ``container escape". Container technologies like Docker and Kubernetes have implemented security measures to mitigate these risks. 

In the software development aspect, cloud providers offer  various of functionalities that can significantly help and optimize the programming processes. They offer the integration services such as Azure Logic Apps which allow to integrate various applications and services. This enables data synchronization between applications, event monitoring, and automated responses to changes \cite{Ecfan:13}.

They offer the cloud databases like Amazon RDS, Azure Cosmos DB, or Google Cloud Firestore  which are flexible and scalable data storage solutions. These solutions offer advanced replication, data backup, and compliance management features \cite{DBS-060:18}.

Cloud networking services empower researchers to build sophisticated, distributed application architectures. Services such as AWS Virtual Private Cloud (VPC), Azure Virtual Network, or Google Cloud Virtual Private Cloud (VPC) allow developers to create isolated networks, manage network traffic, implement advanced security measures, and establish secure connections between different application components \cite{CloudComputing:15}. 

Cloud AI services deliver advanced tools for data analysis, image recognition, natural language processing, and other AI applications. Examples include Azure Cognitive Services, Google Cloud AI Platform, or AWS AI/ML Services. Researchers can leverage pre-trained AI models and algorithms to enrich their applications with intelligent capabilities such as facial recognition, language translation, and sentiment analysis \cite{Akter2022:17}. 

Cloud monitoring and analytics services enable data collection, processing, and analysis of telemetry from various application components. Services like AWS CloudWatch, Azure Monitor, or Google Cloud Monitoring allow developers to track application performance, identify issues, and optimize its operation \cite{LU201992:16}.

Cloud providers also offer  the services which are responsible for the message and event communication between subsystems. One of  them  is Azure Service bus which will be discussed  and used in the communication survey in  the next chapters.

\chapter{Communication}
This chapter will focus on analysis of a problem related to the performed experiments. The conducted research is aimed at demonstrating the effectiveness of a given technology in general terms and the functioning of the entire system.

\section{The role of communication between microservices in the overall perception of a system}
The communication between particular subsystems plays a significant role in functionality of  the entire  system. The communication is used for the following aims: 
\begin{description}
	\item[\textbf{Checks system coherence}] \hfill \\
	Maintaining  the distributed system could be challenging because each subsystem can be written  in a  different time, by various developers and applied by various programming languages. The subsystems exchanges information and validates data again according to the previously predefined futures, to ensure that all the subservices operate within the specified constraints. The validation process checks the consistency of the data and cross-services dependencies and correlations  which guarantees the system coherence.
	\item[\textbf{Database translations integrity}] \hfill \\
	Maintaining the data consistency can be also a challenger to the distributed system. From the definition of microservices, each microservice should have separate instance of database. Even if each of the systems, performs a different operation, it could operate on the same set of data. In this situation if the data is modified by one system, the modifying system should notify other systems to update  their  databases  considering  a  conducted changes.
	
	\item [\textbf{Collaboration of tasks synchronization}]\hfill \\
	Referring to the genesis of microservices architecture, the system is composed of  particular services where each subsystem is responsible for a  single functionality. The whole system collaborates to fulfil the business processes that span across different domains. Communication between them enables the opportunity to work together seamlessly to deliver complex business functionalities. Very often there is a necessity to exchange data in order to perform respective tasks. Communication between microservices plays a significant role in facilitating task synchronization by enabling the exchange of control signals, status updates, and coordination messages among the involved subservices.  Microservices can share the status of updates and progress information during the task execution, enabling other subservices to take appropriate action based on the status. It enables to have real-time transparency, allowing better monitoring and decision-making.
	
	\item [\textbf{Database translations integrity}]\hfill \\
	Maintaining the data consistency can be also a challenger to the distributed system. From the definition of microservices, each microservice should have separate instance of database. Even if each of the systems, performs a different operation, it could operate on the same set of data. In this situation if the data is modified by one system, the modifying system should notify other systems to update  their  databases  considering  a  conducted changes.
	\item \textbf{Fault tolerance}\\
	Communication between microservices plays a considerable role in achieving  an effective  fault tolerance and robust exception handling mechanisms. Microservices are commonly deployed independently and errors could occur only in a separate subsystem. There are various reasons why the errors are made. The main ones include the service unavailability, because of a cloud provider and network disruptions. Microservices can attempt to execute failed operation again providing a change for the success.  They can implement the backoff mechanism,  which can manage the time intervals between attempts simultaneously reducing the time of load of the system and allowing the recovery of  an unavailable system. Additionally, communication enables microservices to implement circuit breakers, which  can detect and prevent further calls to a failing service,  consequently avoiding cascading failures and improving system stability. This isolation enhances the resilience and stability of the microservices architecture.
	
	\item [\textbf{Exception Handling}] \hfill \\
	Exception handling is a crucial aspect of any software system. It coordinates the exception handling across multiple microservices. The communication role in the exception handling in the distributed architecture is propagation the exception in- formation to other relevant services to share the awareness of an error. When an exception occurs in one microservice, it may have implications for related or dependent services It ensures that the exception was properly handled and prevented from being ignored or lost.
	Communication also enables to gain the the insides from the exception properly, for example by sending it to the proper subservice which is responsible for monitoring and collection of system insides. It helps to centralize logging system and monitoring tools. This       can result in  a fast system administrator and developers’ reaction, to repair the system failure.
	
	\item \textbf{Scalability}\hfill\\
	Scalability is main requirement to create modern, efficient software system. Commu- nication between microservices plays a crucial role in achieving scalable architecture.
	Communication enables the horizontal scaling by sharing the information by sub- systems about their availability and capacity  simultaneously allowing the system to adapt and distribute the processes dynamically and  efficiently.  The system can handle higher volumes of requests. Communication supports the concept of elasticity in microservices, which refers to the ability of the  system to scale up and down, automatically based on the demand. By means of communication the  system can monitor the resources usage (such as network bandwidth, CPU and memory usage) and adjust their capacity dynamically. The dynamic scaling helps the efficient utilization of available resources and optimize resources allocation. It also  helps  to prevent bottlenecks.
	
	\item [\textbf{Security}]\hfill\\
	Security is crucial element in all the software systems. Communication plays a significant role in securing systems in microservices architecture.  Communication in microservices exposes them to the risk that send message could be intercepted and be able for thrives to infer the business logic operations. Microservices are commonly deployed in in many distributed containers so the data  so it is a potential threat to steal data from one of them. 
	However the communication process can be under various security mechanisms.
	Implementing the security on communication system can prevent the exposing the malicious process when one of subsystems is attacked.
	Gegick and Barnum 73 proposed that only the minimum necessary rights should be assigned to asubject that requests access to a resource and should be in effect for the shortest duration necessary \cite{https://doi.org/10.1002/cpe.4436:11}. 
	Communication enforces the secure of communication protocols using a network-level security . Microservices can communicate over the secure channels which uses the encryption protocols (for example: Transport Layer Security (TLS) and Secure Sockets Layer (SSL)).
	The encryption makes the communication secure the sensitive data from eavesdropping, tampering, or interception. 
	\\
	To protect data during the communication, there could be implemented the data encryption,  that utilizes shared key and public key encryption. The data can be encrypted by for example Advanced Encryption Standard (AES) as hared key encryption and RSA encryption algorithm as public key encryption scheme.
	There is possibility to implement the Authorization and Authentication  through the communication. Microservices can exchange authentication tokens or credentials to verify the identity of the communicating subsystems. 
	Thanks to the authorization process, there can be implemented additional role management. Role-based access control (RBAC) or attribute-based access control (ABAC), can limit the right to perform specific action only for selected system users. 
\end{description}

\section{Communication Patterns }
In microservices architecture there are a few available communication patterns. The system architect is responsible for analyzing the advantages and disadvantages of each pattern. This evaluation process is crucial in order to make an informed decision and choose the most suitable communication pattern for the specific scenario of the system. Thanks to considering the pros and cons of each pattern, the architect can ensure effective and efficient communication between the services in the system. Ultimately, the chosen communication pattern will play a considerable role in determining the overall performance and functionality of the application.

\subsection{Synchronous}
Synchronous communication refers to a communication pattern where services interact by exchanging messages and wait for a timely response before proceeding further. In this approach, a service initiates a request to another service and waits for the response, blocking its execution until a reply is received. However it is possible to asynchronous interactions using synchronous technologies. Client services instead of waiting for the request to be completed, can implement the pattern like pooling in separate thread to check the service for completion in some period of time. The examples of synchronous communication protocols are as follows:
\begin{itemize}
	\item Hypertext Transfer Protocol (HTTP),
	\item Transmission Control Protocol (TCP),
	\item Remote Procedure Call (RPC),
	\item Simple Object Access Protocol (SOAP).
\end{itemize}

\begin{figure}[h]
	\centering
	\includegraphics[width=0.7\textwidth]{communication.png}
	\caption{Synchronous communication diagram}
	\label{fig1}
\end{figure}

\subsection{Asynchronous}
Asynchronous communication refers to a communication pattern where services interact without requiring immediate and direct response. In this approach, a service sends a message or request to another service and continues its execution without waiting for a response. The receiving service processes the message independently and may respond at a later time or not at all. \\

The asynchronous communication usually is  carried out  by implementing the message broker. Below there is a description of used terms in following communication patterns:
\begin{description}
	\item \textbf{Message Broker}\\
	Message broker is a piece of  the software which facilitates a data queue that connects the Producer and Consumer services.  
	\item \textbf{Producer}\\
	Application which is  responsible for sending or even also generating messages. It sends data directly to the message broker. In publish-subscribe pattern they are called publishers.  
	\item \textbf{Consumer}\\
	Application which consumes the data from the message broker. In the  published/subscribed pattern they are called subscribers
	\item \textbf{Message}\\
	Message is the objects sent by producer to message broker. Message is composed by a header (the metadata used for the message indication or security information) and body. 
	\item \textbf{Queue/topic}\\
	A directory where messages are stored.
\end{description}
The message could be consumed from the message broker by single or multiple Consumers. If a consumer wants to send  a response after processing  the data, it can sent  the message back to the message broker which will be subsequently caught by publisher system. \\
The most popular protocols for the Asynchronous Communication are
\begin{itemize}
	\item Message Queuing Telemetry Transport (MQTT),
	\item Advanced Message Queuing Protocol (AMQP),
	\item Apache Kafka,
	\item Java Message Service (JMS),
	\item WebSockets.
\end{itemize}
\begin{figure}[h]
	\centering
	\includegraphics[width=0.7\textwidth]{communication.png}
	\caption{Asynchronous communication diagram}
	\label{fig1}
\end{figure}
\section {Available technologies}
Currently there is a huge diversity of different technologies available on the market   which are responsible for the connection of multiple subsystems. All of the commonly used technologies are based on the communication using the protocols listed above.
\subsubsection{Synchronous}
\begin{description}
	\item Hypertext Transfer Protocol (HTTP)
	\begin{itemize}
		\item RESTful
		\item GraphQL
		\item Azure Service Bus.
	\end{itemize}
	\item Transmission Control Protocol (TCP)
	\begin{itemize}
		\item RMI
		\item gRPC 
	\end{itemize}
	\item Remote Procedure Call (RPC)
	\begin{itemize}
		\item gRPC
	\end{itemize}
	\item Simple Object Access Protocol (SOAP)
	\begin{itemize}
		\item Apache CXF 
		\item WCF  
	\end{itemize}
\end{description}
\subsubsection{Asynchronous}
\begin{description}
	\item Message Queuing Telemetry Transport (MQTT)
	\begin{itemize}
		\item Eclipse Paho
	\end{itemize}
	\item Advanced Message Queuing Protocol (AMQP)
	\begin{itemize}
		\item RabbitMQ
		\item Azure Service Bus
	\end{itemize}
	      
	\item Apache Kafka
	\begin{itemize}
		\item Kafka Streams
		\item Confluent Platform
	\end{itemize}
	\item Java Message Service (JMS)
	\begin{itemize}
		\item Apache ActiveMQ
		\item Apache IBM MQ
	\end{itemize}
	\item WebSockets  
	\begin{itemize}
		\item Socket.IO
		\item SignalR
		\item Azure Service Bus
	\end{itemize}
	   
\end{description}
\section{Communication patterns used in the scope of the experiments}
\subsection{Azure Service Bus}
\label{asbdefinitions}

Azure bus service platform-as-a-service fully managed enterprise message broker with message queues and publish-subscribe topics on the Azure Cloud. 
\begin{description}
	\item[\textbf{Queue}] \hfill \\
	Messages are ordered in queue and timestamped with arrival time. It provides the First In, First Out (FIFO) message delivery to one or more competitive customers.  Service keeps messages until they have been reported by client as accepted. Queue usually is used as a point-to-point communication, because each message can be processed only by one customer. 
	\cite{11}
	\item[\textbf{Topic}] \hfill \\
	Topic functionality in reference to queue, is extended by the possibility of having multiple subscribers who are listen to it.  It is suitable for the publish/subscribe pattern which is useful for scaling huge number of recipients. Each message is available for each client who is subscribes the topic. The message is deleted after every subscriber has processed the message. \cite{11} Topics can possess multiple properties that clients can utilize for filtering, whereas queues do not implement filtration options.
	\item[\textbf{Subscription}] \hfill \\A topic subscription, is strictly related to the topic. This is a mechanism in Azure Service Bus that allows multiple consumers to receive copies of messages sent to a topic. It functions as  a virtual queue, where each subscription acts as an independent recipient of messages. When a message is sent to the topic, it is automatically forwarded to all associated subscriptions.
\end{description}

\subsubsection{Formats}
The messages could be constructed in following formats:  JSON, XML, Apache Avro, Plain Text. 

\subsubsection{Protocols}
The messages is Azure Service Bus can be sent by the following protocols:
AMQP 1.0/1.1 (Advanced Message Queuing Protocol), MQTT 3.1 (MQ Telemetry Transport), OpenWire 2, 0MQ 2, STOMP 1.2, or HTTP 1.1. The Advanced Message Queuing Protocol is default protocol used by Azure Service bus.

\subsection{RabbitMQ}
\label{RabbitMQdefinitions}
RabbitMQ is an open source message broker which can be deployed in clustered con- figurations in any environment to meet high availability and throughput needs. RabbitMQ supports the Advanced Message Queuing Protocol(AMQP) and MQ Telemetry Transport (MQTT). Advanced Message Queuing Protocol is an programmable protocol which im- plies these  entities and routing schemes primarily defined by applications, rather than by a broker administrator. Accordingly, provision is made for protocol operations that declare  queues and exchanges, define bindings between them, subscribe to queues and so on. [ ref https://www.rabbitmq.com/tutorials/amqp-concepts.html]
The messages published by a publisher are not directly transferred to the queue but to the exchange.The Exchange is responsible for routing messages to a  different  queue using binding, which is a link between a queue and an exchange, and routing keys, which is an message attribute added to the message header.


\paragraph{Exchanges Types}\mbox{} \\
Exchanges take message and route into zero or more queues, depending on the exchange type and bindings.
\begin{description}
	\item[\textbf{Direct exchange}] \hfill \\
	It uses the message routing key to transport messages into the single queue. If the message routing key does not match any binding key, the message is discard.
	\item[\textbf{Default exchange}] \hfill \\
	A default exchange is an direct exchange with no name, pre-declared by the broker. It has one particular feature that makes it useful for simple applications: every  created queue is automatically bound to it using a routing key that matches the queue name. For instance, if a queue is declared with the name "search-indexing- online," the AMQP 0-9-1 a broker will automatically bind it to the default exchange using "search-indexing-online" as the routing key. As a result, any message published to the default exchange with the routing key "search-indexing-online" will be routed to the queue "search-indexing-online."
	\item[\textbf{Topic exchange}] \hfill \\
	It is similar to the direct exchange however topic exchange instead of  using  a fixed routing key,  it uses wildcards. The message is routed to one or many queues, basing on a matching between a message routing key and  a pattern. A routing pattern of “university.*.*.technology” only matches  routing keys where the first word is “university" and the fourth word is “technology”.
	\item[\textbf{Fanout exchange}] \hfill \\
	Fanout exchange routes message to all the queues bounded to it  ignoring the routing key. It is ideal for the broadcast routing of messages. The exemplary usage of this exchange is the system where the same messages should be caught  by a mobile application and a  web application or group chats where messages are distributed among participants. 
	\item[\textbf{Header exchange}] \hfill \\
	Header exchange uses routing on multiple attributes that are more easily expressed as a  message header than as a routing key. It is similar to the topic exchange however it uses the header attribute instead of the routing key, which is here ignored.  A Specific argument termed “x-match” indicates whether all  the headers must match or only one queue. The headers can be built not only with  a string but also with types such as integers or hashes.
\end{description}
\subsection{REST API}
REST is an acronym which stands for Representational State Transfer. An API an acronym which denotes  Application Programming Interface. This is defined  as  a set of rules, commands, permissions, or protocols allowing application to interact with other applications. A REST API is based on representational state transfer (REST) and offers a set of guideline that software can use to communicate over the internet in order to create integrations between  the systems. A RESTful API uses commands to obtain resources. Is takes advantage of  existing HTTP methodologies defined by the RFC 2616 protocol (Hypertext Trans- fer Protocol).  A Set of definitions related to the REST API: 
\begin{description}
	\item[\textbf{Client}] \hfill \\
	The client makes requests to the API in order to retrieve specific information or initiate changes within the application. This can involve the operations of fetching or submitting data.
	
	\item[\textbf{Resource}] \hfill \\
	A resource refers to any piece of information that the API can provide to the client. It can represent data entities, files, functionalities, or any other element that the client interacts with via the API.
	
	\item[\textbf{Server}] \hfill \\
	The server is a fundamental component utilized by the application to handle in- coming client requests. It stores and manages the resources that the client interacts with. The server exposes an API that acts as an intermediary layer, enabling clients to communicate with the application’s functionalities while maintaining a controlled access to the underlying database content.
	
	\item[\textbf{Endpoint}] \hfill \\
	An endpoint is a specific URL (Uniform Resource Locator) that the client uses to send requests to the server's API. Each endpoint corresponds to a distinct resource or functionality within the Server.
	
	\item[\textbf{HTTP Methods}] \hfill \\
	HTTP methods, such as POST, GET, PUT and DELETE, used  to perform create, retrieve, update, delete operations, are  commonly known as CRUD respectively. They define the action to be taken when making requests to specific API endpoints.
	
	\item[\textbf{Data Format}] \hfill \\
	APIs use standardized data formats like JSON (JavaScript Object Notation) or XML (eXtensible Markup Language) to structure and transmit data between the client and the server.
	
	\item[\textbf{Authentication and Security}] \hfill \\
	APIs often employ authentication mechanisms, such as API keys or OAuth tokens, to verify the identity of clients before granting an  access. The credentials are sent  over the http request defined in  the header.
	
\end{description}

\chapter{Implementation}
This chapter provides details about the use-case application that has been implemented in order to evaluate the impact technology to the communication between microservices. It comprises a  brief description of the system architecture and the functionalities which are served by each microservice in order to conduct experiments. Due to  the necessity of customize implementation for different experiments,  any information related to the exact implementation of the functionality can be found under the \nameref{chap:experiments} chapter.

\section{Application}
A system in microservice architecture has been implemented, in order to simulate a real traffic between subsystems. The goal is to fetch messages produced by the producer’s application from the clients applications. This simulation is essential for evaluating the system’s behavior under different conditions, ensuring its reliability, and fine-tuning performance. The objective is to fetch messages generated by the producer’s application efficiently. The whole system is implemented in the .NET 6 framework. Thanks to the .NET being a cross platform framework,it is a suitable solution for building microservices, because  it can be running  on various operating systems, enhancing portability and flexibility. It offers powerful tools for building WEB API’s and multiple libraries for implementing the integration with various external services. It is crucial for integration with  The RabbitMQ message broker and Azure services. 
\subsection{Architecture}
The project is a microservices-based application developed in C\#, where each microservice is implemented as a separate ASP.NET Core application using .NET 6. The application is designed to provide a scalable and modular architecture, allowing independent development and deployment of each microservice. Each microservice is deployed as a single container on the docker local environment.

Containers ordinarily get their own private network that is separate to the host stack. However in order to use inter-container networking to carry out the communication between them, the  containers are allowed to share  the  host’s network stack. It means that they are allowed to access the host machine’s localhost instead of the container’s host itself.

The figure BLABLA provides an overview of data flow in the  whole system. Once the data is collected and processed by the publisher application, it is resent to the particular data stores. Then the data is caught  by the  client applications and sent  via websocket to the  virtual machine over the websocket.

Whole system consists of :


\subsection{Publisher API}
Producer application is a microservice which is provided with a set of data. In reference to the microservices principles, there is no option to share common database within subsystems. The primary objective of the application is to efficiently process the services data with clients, using RESTful and asynchronous communication methods. The  Producer dispatches data to  a designated message brokers: RabbitMQ broker, Azure Service Broker and saves it in the local database. In a REST architecture, client applications directly receive data from the Producer API. Thus, the Producer API needs a storage mechanism for data until requested by the REST Client API. When RESTClient microservice requests access to previously processed data , the Producer Application retrieves  data from the database and delivers it synchronously to the clients in response to HTTP requests. The producer functionalities are invoked by the HTTP Request to listed endpoint below.
\paragraph{Produce Data Controller}\mbox{} \\
This controller manages data production and distribution.
\begin{description}[style=nextline, font=\normalfont\textbf]
	\item [HTTP POST] \textnormal{\texttt{host/api/producer/produce/rabbitMQ/direct}}
	This endpoint triggers the process of publishing a dataset to a RabbitMQ direct exchange with pre-configured bindings.
	\item [HTTP POST] \textnormal{\texttt{host/api/producer/produce/rabbitMQ/fanout}}
	Invoking this endpoint initiates the publishing process of a dataset to a RabbitMQ fanout exchange with pre-configured bindings.
	  
	\item [HTTP POST \texttt{host/api/producer/produce/azureServiceBusQueue}]
	When this endpoint is called, the process of publishing a dataset to an Azure Service Bus Queue begins
	
	\item [HTTP POST \texttt{host/api/producer/produce/azureServiceBusTopic}]
	Calling this endpoint triggers the process of publishing a dataset to an Azure Service Bus Topic.
	
	\item [HTTP POST \texttt{host/api/producer/produce/database}]
	When this endpoint is invoked, data is saved to the local database. This process is particularly relevant for synchronous REST communication. 
\end{description}
\paragraph{REST Data Provider Controller}\mbox{} \\
This controller facilitates the communication with REST Client API. 
\begin{description}[style=nextline, font=\normalfont\textbf]
	\item [HTTP GET \texttt{host/api/producer/RESTDataProvider/GetById/{id}}]
	When the REST Client API requires a specific piece of data, it invokes this endpoint and includes the relevant objectId in the request parameters. The Producer then accesses the database, retrieves the corresponding object, and responds with it to the request.
	
	\item [HTTP GET \texttt{host/api/producer/RESTDataProvider/GetAll}]
	To retrieve all the produced data, the REST Client API utilizes this endpoint. By making a request to this URL, the REST Client API prompts the Producer to retrieve all objects stored in the database. The Producer then compiles these objects into a list and sends the list as a response to the request.
\end{description}

Figure no [] provides the architecture schema of Producer API. The Producer API is deployed as a solitary container within the Docker localhost environment.

\subsection{REST Client API}
The rest client API is an application is built on ASP.NET Core technology, to retrieve data from the Producer API over the HTTP protocol using REST API principles.The application serves as a data consumer and interacts with Producer API to obtain required data in  a synchronous manner. Rest Client API sends HTTP GET requests directly to the Producer API. Upon a successful response from the Producer API, the application receives the requested data. In case of any error during the API call,  the application handles the exception and returns it with a suitable status code. In order to simulate multiple consumers traffic for experiment no 2, the instance of REST Client API, has been deployed 5  five times in form of separated docker containers.
 
The REST Client API has been designed to facilitate communication with external clients through the HTTP protocol. To initiate interactions, the following endpoints have been established:

\begin{description}[style=nextline, font=\normalfont\textbf]
	\item [HTTP GET \texttt{host/api/RestClient/{id}}]
	This endpoint initiates the process of retrieving individual data from the Producer API. In response, the application returns a single message.
	
	\item [HTTP GET \texttt{host/api/RestClient/all}]
	This endpoint initiates the process of retrieving all produced data from the Producer API. In response, the application returns a list of messages.
\end{description}
The REST Client API is deployed as a solitary container within the Docker localhost environment.
\subsection{RabbitMQ Consumer APIs}
RabbitMQ Consumer API is an application built on ASP.NET Core technology, to retrieve data produced by Producer Api using RabbitMQ message broker over the AMQP protocol. 

It does not interact with  the producer API directly, but rather they communicate via an intermediary known as message broker. RabbitMQ message broker is deployed as a separate container on the local environment, using Docker image for Rab- bitMQ. The RabbitMQ Docker image is based on the official RabbitMQ image available on Docker Hub, and it is created using a Dockerfile. The Dockerfile defines the instructions to build the image, including the base image, environment setup, and any additional configurations needed f50or RabbitMQ.
Once the RabbitMQ Consumer API is in the need of data, it makes a  call to the message broker with pre-configured binding to pick it up. For the purpose of conducting two different experiments with the use of the RabbitMQ message broker, two versions of the RabbitMQ Consumer API application have been  created. The versions differ in the type of exchange they refer to.
\subsubsection{RabbitMQ Concumer Direct Exchange API}\
In the context of the experiment described in \ref{Experiment1} wherein a single consumer receives the data, the RabbitMQ Consumer API retrieves data from the direct exchange.
Referring to the definitions presented in \ref{RabbitMQdefinitions}, the direct exchange sends the data directly to the queue specified in binding key.
The RabbitMQ Consumer Direct Exchange API has been designed in order to enable  communication  with external clients using the HTTP protocol. To obtain the interaction, the following endpoints have been made;
\begin{description}[style=nextline, font=\normalfont\textbf]
	\item [HTTP GET \texttt{host/api/rabbitMQConsumer/direct/single}]
	This endpoint triggers the process of retrieving a single message from the RabbitMQ broker. The application responds with the retrieved message.
	
	\item [HTTP GET \texttt{host/api/rabbitMQConsumer/direct/all}]
	This endpoint initiates the process of collecting all available messages from the RabbitMQ broker. In response, the application provides a list of messages.
\end{description}

\subsubsection{RabbitMQ Consumer Fanout Exchange API}
For the \nameref{Experiment2} where multiple consumers recive data simultaneously, the RabbitMQ consumer API, collects data from Fanout Exchange. 
Since RabbitMQ only supports queuing logic, the  messages are enqueued and dequeued in the First-In First-Out manner. For the multiple consumers scenario, there is an need of enqueue messages to the multiple queues, each for one consumer. The fanout exchange is suitable in this case, because it routes messages to all of the queues that are bound to it. 

The RabbitMQ Consumer Direct Exchange API has been designed to facilitate communication with external clients through the HTTP protocol. To initiate interactions, the following endpoints have been established:
\begin{description}[style=nextline, font=\normalfont\textbf]
	\item [HTTP GET \texttt{host/api/rabbitMQConsumer/fanout/single}]
	This endpoint triggers the process of retrieving a single message from the RabbitMQ broker using a fanout exchange. The application responds with the retrieved message.
	
	\item [HTTP GET \texttt{host/api/rabbitMQConsumer/fanout/all}]
	This endpoint initiates the process of collecting all available messages from the RabbitMQ broker using a fanout exchange. In response, the application provides a list of messages.
\end{description}

\subsection{Azure Service Bus APIs}
Azure Service Bus Queue Consumer API is an application built on ASP.NET Core technology, to retrieve data produced by the Producer API using Azure Service Bus cloud message broker over the AMQP protocol. 

Referring to the possibilities offered by Azure, and the lack of recommendations related to the selection of the appropriate solution by Microsoft, the process of collecting data from both queues and topics has been implemented. Each pattern is implemented by the separate microservice.
\subsubsection{Azure Service Bus Queue API}

Azure Service Bus Queue Consumer API substitute of RabbitMQ Consumer API in the context of messaging systems. It also collects data from  the queue, using pattern First- in-First-out although Azure Service Bus is an cloud-base message broker as opposed to the traditional local deployment of RabbitMQ. Once the Azure Service Bus Consumer API is in the need of data, it makes a  call to the message broker with pre-configured queue binding.
The Following API is used in the \nameref{Experiment1}. In the reference to the definitions provided in \nameref{asbdefinitions}, the queue is often used to point-to-point communication, because of the mentioned first-in first-out manner. It is suitable for a single consumer scenario. 

The Azure Service Bus Queue Consumer API has been designed to facilitate communication with external clients through the HTTP protocol. To initiate interactions, the following endpoints have been established: 
\begin{description}[style=nextline, font=\normalfont\textbf]
	\item [HTTP GET \texttt{host/api/asbConsumer/queue/single}]
	This endpoint triggers the process of retrieving a single message from the Azure Service Bus broker queue. The application responds with the retrieved message.
	
	\item [HTTP GET \texttt{host/api/asbConsumer/queue/all}]
	This endpoint initiates the process of collecting all available messages from the Azure Service Bus broker queue. In response, the application provides a list of messages.
\end{description}
    
\subsubsection{Azure Service Bus Topic API}
In   \nameref{Experiment2} the following API is utilized. The definitions referred in \nameref{asbdefinitions}, indicate that the topics are appropriate for multi-consumer scenario. This is because a topic facilitates message sharing with all subscribers who are listen to the particular topic. 

To implement this functionality, five docker containers of Azure Service Bus Topic API have been deployed. Each container is distinct from the others based on its reference to a different Azure Service Bus subscription.
 
When the Azure Service Bus Consumer API requires  any data, it initializes a call to the  message broker under an adequately   implemented subscription. This subscription has a pre-configured binding to the appropriate topic, allowing the API to retrieve the data.

The Azure Service Bus Consumer API has been designed to facilitate communication with external clients through the HTTP protocol. To initiate interactions, the following endpoints have been established:

\begin{description}[style=nextline, font=\normalfont\textbf]
	\item [HTTP GET \texttt{host/api/asbConsumer/topic/single}]
	This endpoint triggers the process of retrieving a single message from the Azure Service Bus broker topic. The application responds with the retrieved message.
	
	\item [HTTP GET \texttt{host/api/asbConsumer/topic/all}]
	This endpoint initiates the process of collecting all available messages from the Azure Service Bus broker topic. In response, the application provides a list of messages.
\end{description}

\chapter{Experiments} 
\label{chap:experiments}
The "Experiments" chapter provides the description of 3 different scenarios, explanaion how  a chosen  communication pattern could be implemented in the Web Application source code, what difficulties and restrictions  could be met and the analysis of the metrics collected during carrying out  the experiments. Each experiment is aimed to to detect  the advantages and disadvantages of using the selected communication pattern for a specific use case.

\section{Metrics taken into account during the analysis}

Metrics for following experiment have  been collected using The Apache JMeter™ tool. This is an open software toll, based on Java, to perform a  load and  a functional test. It helps to simulate multiple scenarios such as heavy load on a server to analyse  an  overall performance under different load conditions. It helped to get the  insides of a  request latency, load time, throughput, minimum and maximum time of response,number of errors,  the size of objects and  a general experiment time. Thanks to that, there was an  opportunity to analyse the results in a  clear way The diagrams showing the data acquisition in a transparent an clear way have been generated. 

\begin{itemize}
	\item \textbf{Latency} \\
	      Latency refers to the time difference between when a request is sent and the moment the response begins to be received.
	      The average latency is given by:
	      \[
	      	\text{Average latency} = \frac{\text{Sum of latencies}}{\text{Total number of events}}
	      \]
	\item \textbf{Connect Time} \\
	      Connect time defines the time taken to establish the TCP connection between the client and the server using TCP Handshake. This process involves the initial steps of a three-way handshake: the client sends a SYN packet to the server, the server responds with a SYN-ACK packet, and finally, the client acknowledges the server's response with an ACK packet. Whereas TCP Handshake is successful, then the client can send further requests. If not, the client can't talk to the server. the server might not be available, couse of beeing overwhelmed with other requests, or there could be network issues preventing the handshake from completing,
	      
	\item \textbf{Average response time} \\
	      Response time signifies the time difference between when a request is sent and when the complete response is received. The response time should be always longer than latency. The larger response body is, the larger discrepancy  between response time and latency is.
	      The average response time is given by:
	      \[
	      	\text{Average response time} = \frac{\text{Sum of response times}}{\text{Total number of requests}}
	      \]
	\item \textbf{Throughput} \\
	      Throughput denotes the total count of requests that an application can process during a specified time frame. In contrast to Latency, the higher throughput value indicates  a better performance and system capacity to handle multiple requests concurrently. In the metrics below, throughput is measured within one second. Thoughput is calculated by following formula: \\
	      \[
	      	\text{Throughput (RPS)} = \frac{\text{Number of Requests}}{\text{Test Duration (in seconds)}}
	      \]
	\item \textbf{90\% Line (90th Percentile)} \\
	      The response time below which 90\% of the samples fall,
	\item \textbf{Error rate} \\
	      The error rate represents the number of errors that occurred during the experiment,
	\item \textbf{standard deviation } \\   
	      A standard deviation is a measurement of how data is distributed in relation to the mean. Low standard deviation indicates that data is clustered around the mean. Nonetheless the high standard deviation indicates that data is more distributed. When a standard deviation approximates  zero it signifies that data points are close to the mean. A high or low standard deviation indicates data points are respectively above or below the mean.
	      The standard deviation is calculated by:
	      \[
	      	\text{Standard deviation} = \sqrt{\text{Variance}}
	      \]
	      The variance is calculated by:
	      \[
	      	\text{Variance} = \frac{1}{N} \sum_{i=1}^{N} (x_i - \text{Mean})^2
	      \]
	      Where:
	      \begin{itemize}[label=--]
	      	\item \(\text{Variance}\) represents the variance.
	      	\item \(N\) is the total number of data points.
	      	\item \(x_i\) represents each individual data point.
	      	\item \(\text{Mean}\) represents the mean (average) of the data points.
	      	\item \(\sum_{i=1}^{N}\) denotes the summation over all \(N\) data points.
	      \end{itemize}
\end{itemize}

\section{ First experiment}
\label{Experiment1}
"The client application wants to use the data already processed by the producer application. The data produced by the producer application is placed in various resources, depending on the communication technology (REST, RabbitMQ, Azure Service Bus) that microservices use. The producer application does not need to get information about whether the data has been received by the client. The customer receives the data when   he needs it, so that he does not  have to store it in his system unnecessarily.  The Data produced by producer has  a single consumer" 

\subsection{Key values of the scenario}
\begin{enumerate}[label=\arabic*.]
	\item \textbf{Asynchronous:} The communication between the producer application and the client application is asynchronous. The client does not wait for the data to be immediately available upon request but retrieves it at a later time.
	          
	\item \textbf{Decentralized Data Placement:} The data produced by the producer application is placed in various resources, depending on the communication technology (REST, RabbitMQ, Azure Service Bus) that microservices use. This implies a decentralized approach to data storage and placement.
	          
	\item \textbf{Unidirectional Communication:} The data flows from the producer to the consumer (client) in a unidirectional manner for Azure Service bus and RabbitMQ. The producer does not need to know whether the data has been received by the client.
	          
	\item \textbf{On-Demand Data Consumption:} The client receives the data when he/she needs it, ensuring that it does not store unnecessary data locally. This efficient data consumption approach minimizes storage overhead on the client side
	          
	\item \textbf{Single Consumer:} The data produced by the producer has a single consumer, which is the client application. This indicates a one-to-one relationship between the producer and the consumer for the specific dataset being retrieved.
\end{enumerate}


The experiment was divided into two variants:
\begin{itemize}
	\item The client receives the data individually, one request for each data,
	\item The customer receives all the data available within one request;
\end{itemize}
For each variant has been performed experiments on different amount of object: 
\begin{itemize}
	\item 1000 objects, 
	\item 300 000 objects;
\end{itemize}
For each variant has been performed experiments on different size of object: 
\begin{itemize}
	\item UTF-8 encoded string created by joining various properties of the \texttt{joystic} object, resulting in a size of 47 bytes.
	\item -----;
\end{itemize}

\subsection{ First experiment technical arrangement}
\label{sec:firstexperimenttechnicalarrangement}
\subsubsection{Publisher}
\label{subsec:publisherfirst}
\paragraph{Azure Service Bus Queue Publisher}\mbox{} \\
\label{asbPublisher}
Configuring the function for sending data to the Azure Service Bus was a highly intricate process. The focus while implementing the function was put on expeditiously sending considerable data packets to the Azure Service Bus. The connection with Azure Service Bus is made using Nuget package \texttt{Azure.Messaging.ServiceBus}.There is created an object of \texttt{ServiceBusClient} and \texttt{ServiceBusSender}, and by means of connection string and queue name. 
The code ~\ref{lst:azurepublisherqueuesnippet} presents the process of publishing messages to the Azure Service Bus. The publishing procedure was optimized by structuring messages into batches that conform to predefined size constraints. The utmost size of each message batch is constrained to 256 KB, and when the size is exceedted it results in batch rejection. Prescribed maximum batch size is 256 KB, however the size of message is calculated taking into account not only the message body size, but also the headers and other message metadata. Function  \texttt{TryAddMessage}, attempts to add a message to the batch, ensuring that the size of the batch does not exceed its maximum. Otherwise,  the function does not add the message to the batch, it sends the current batch to Azure Service Bus, disposes it, recreates a  new one and finally adds the  message to  the recent batch. The process is repeated in each iteration while exceeding the end of the list of objects waiting to be sent.

\begin{lstlisting} [float=h!,caption={Code in C\# presenting the process of sending messages to the queue on the Azure Service Bus}, label={lst:azurepublisherqueuesnippet}]
var serviceBusMessageBatch = await sender.CreateMessageBatchAsync();

for (int i = 0; i < message.Count; i++)
{
    var messageBytes = Encoding.UTF8.GetBytes(String.Join(",", message[i].time, message[i].axis_1, message[i].axis_2, message[i].button_1,
        message[i].button_2, message[i].id.ToString()));

    if (!serviceBusMessageBatch.TryAddMessage(new ServiceBusMessage(messageBytes)))
    {
        await sender.SendMessagesAsync(serviceBusMessageBatch);
        serviceBusMessageBatch.Dispose();
        serviceBusMessageBatch = await sender.CreateMessageBatchAsync();
        serviceBusMessageBatch.TryAddMessage(new ServiceBusMessage(messageBytes));
    }
}
await sender.SendMessagesAsync(serviceBusMessageBatch);

\end{lstlisting}
\paragraph{RabbitMQ Direct Exchange Publisher}\mbox{} \\
The process of interacting with RabbitMQ involves sending and consuming messages. To send messages, the connection to RabbitMQ is established by the Nuget package \texttt{RabbitMQ.Client}. This is achieved by creating an object of \texttt{ConnectionFactory} and \texttt{IModel}, which represents the communication channel. The connection is made to the address where the Message Broker is located. Channels are utilized to interact with RabbitMQ queues. Within a channel, a queue is declared, defining attributes such as the queue name, durability, exclusivity, and auto-deletion.
The code ~\ref{lst:rabbitsnippet} demonstrates how messages are sent to direct exchange on RabbitMQ message broker. Each message, in contrast to Azure Service Bus, is published separately to the specified exchange (default exchange) with the routing key set to predefined \texttt{\_queueName}. 

\begin{lstlisting} [float=h!,caption={Code in C\# presenting the process of sending messages to the direct exchange on RabbitMQ}, label={lst:rabbitsnippet}]
channel.QueueDeclare(queue: _queueName,
                               durable: false,
                               exclusive: false,
                               autoDelete: false,
                               arguments: null);

foreach (Joystick Joystick in message)
{
    channel.BasicPublish(exchange: "",
                         routingKey: _queueName,
                         basicProperties: null,
                         body: Encoding.UTF8.GetBytes(String.Join(",", Joystick.time, Joystick.axis_1, Joystick.axis_2,Joystick.button_1, Joystick.button_2, Guid.NewGuid().ToString())));
}
\end{lstlisting}

\paragraph{REST}\mbox{} \\
In the context of implementing the data communication functionality through REST communication pattern, the data is persistently stored within a designated database repository. The code ~\ref{lst:restsavedata} presents the essential steps of saving data to the database. The code begins by constructing an SQL query template for inserting data into the 'Joystics' table. The query employs parameter placeholders for each attribute, ensuring secure and parameterized database interactions to prevent SQL injection vulnerabilities. Within the loop, the code iterates through each `Joystic` object extracted from the REST communication. For each `Joystic` object, the code sets the corresponding parameter values with the attributes of the object. Upon setting the parameter values, the code executes the SQL command using `ExecuteNonQuery()`. This action inserts the data from the `Joystic` object into the database as a new record. It is noteworthy that the entire operation takes place  within a single transaction, as indicated by the `BeginTransaction()` and `Commit()` methods. It helps to get better performance, than committing each row separately.  This transactional approach ensures data integrity and consistency. Either all records are inserted successfully, or none is inserted in the case of an error.
\begin{lstlisting} [float=h!,caption={Code in C\# presenting the process of saving all objects to the local database enginee.}, label={lst:restsavedata}]
string insertQuery = "INSERT INTO Joysticks (Time, Axis_1, Axis_2, Button_1, Button_2) VALUES (@Time, @Axis_1, @Axis_2, @Button_1, @Button_2)";

using (SQLiteCommand cmd = new SQLiteCommand(insertQuery, this.sqlite_conn))
{
    cmd.Parameters.Add(new SQLiteParameter("@Time"));
    cmd.Parameters.Add(new SQLiteParameter("@Axis_1"));
    cmd.Parameters.Add(new SQLiteParameter("@Axis_2"));
    cmd.Parameters.Add(new SQLiteParameter("@Button_1"));
    cmd.Parameters.Add(new SQLiteParameter("@Button_2"));

    using (var transaction = this.sqlite_conn.BeginTransaction())
    {
        foreach (Joystick Joystick in Joysticks)
        {
            cmd.Parameters["@Time"].Value = Joystick.time;
            cmd.Parameters["@Axis_1"].Value = Joystick.axis_1;
            cmd.Parameters["@Axis_2"].Value = Joystick.axis_2;
            cmd.Parameters["@Button_1"].Value = Joystick.button_1;
            cmd.Parameters["@Button_2"].Value = Joystick.button_2;

            cmd.ExecuteNonQuery();
        }

        transaction.Commit();
    }
}
\end{lstlisting}

\subsubsection{RabbitMQ Direct Exchange Client API}
\label{subsec:rabbitMQFirst}
\paragraph{Retrieving individual messages in a single request}\mbox{} \\
The process of consuming a single message from a direct exchange using the RabbitMQ library is shown in \ref{lst:rabbitmqconsumedirectsinglesnippet}. The procedure begins with instantiating a \texttt{ConnectionFactory} object, which encapsulates the necessary details for establishing connections to the messaging system. Once a connection is established, a communication channel is created. Within this channel, a queue is declared with specified attributes, mirroring the message-sending process.The connection is established through the IRabbitMQConnectionFactory service, maintains a singleton lifecycle within the application. This design ensures that, the connection is established once per application life time, preventing  the scenario where, each request triggers new connection to RabbitMQ. This optimalization helps to enhance processing efficiency, by avoiding unncessary slowdowns.

To facilitate a message retrieval, an event-based consumer is created on the communication channel. The subsequent code segment endeavors to fetch messages using the \texttt{BasicGet} method, incorporating parameters for a queue name and auto-acknowledgment. Auto-acknowledgment signifies automatic marking of messages as received. When a message is retrieved, its byte-array body is transformed into a coherent string representation using UTF-8 encoding.
\begin{lstlisting} [float=h!,caption={Code in C\# presenting the process of consuming single message in one request from RabbitMQ message broker}, label={lst:rabbitmqconsumedirectsinglesnippet}]
{
    _channel.QueueDeclare(queue: _queueName, durable: false, exclusive: false, autoDelete: false, arguments: null);
    var consumer = new EventingBasicConsumer(_channel);
    BasicGetResult result = _channel.BasicGet(_queueName, autoAck: true);
        if (result != null)
        {
            var data = Encoding.UTF8.GetString(result.Body.ToArray());
            return Ok(data);
        }
        else
        {
            return NotFound("No data available in the queue.");
        }
}
\end{lstlisting}
    
\paragraph{Retrieving all messages in a single request}\mbox{} \\
The pivotal steps involved in consuming messages from a direct exchange using the RabbitMQ library are outlined in code the ~\ref{lst:rabbitmqconsumedirectallsnippet}.Likewise, in the single message retrieval approach, the code initiation involves creating instances of \texttt{ConnectionFactory} and channel objects. Subsequently, a queue is declared with specific attributes.

To accommodate the retrieved message data, a collection named \texttt{dataList} is initialized. The code employs an iterative loop mechanism to retrieve messages continuously  from the designated queue.  During each iteration, the \texttt{BasicGet} method is employed to retrieve a message, and an auto-acknowledgment mechanism is invoked. Upon successfully obtained  a message, its byte-array body is transformed into a coherent string using UTF-8 encoding, and subsequently added to the \texttt{dataList} collection.When no messages are retrieved, the loop ends, acknowledging the absence of additional data in the queue. 
\begin{lstlisting} [float=h!,caption={Code in C\# presenting the process of consuming all messages in one request from RabbitMQ message broker}, label={lst:rabbitmqconsumedirectallsnippet}]
{
_channel.QueueDeclare(queue: _queueName, durable: false, exclusive: false, autoDelete: false, arguments: null);
var dataList = new List<string>(); 
    while (true)
    {
        BasicGetResult result = _channel.BasicGet(_queueName, autoAck: true);
        if (result != null)
        {
            var data = Encoding.UTF8.GetString(result.Body.ToArray());
            dataList.Add(data);
        }
        else
        {
            break;
        }
    }
}
\end{lstlisting}
\subsubsection{REST Client API}
\label{subsec:restfirst}
\paragraph{Retrieving individual messages in a single request}\mbox{} \\

The consumption of a solitary message from a producer application, employing the REST communication pattern is presented in the code ~\ref{lst:restclientbysinglesnippet}. An asynchronous HTTP GET request is sent to an external Producer API endpoint to retrieve a single message. The request URL is constructed using the provided message \texttt{id} parameter.
The code ~\ref{lst:restclientpublishersnippet} presents how the Producer API catches the request and responses with message acquired  by specific \texttt{id} from database . 
A client after receiving  the response, reads the content of the response as a string and if the response is successful, the retrieved data is returned to a requestor. 

\begin{lstlisting} [float=h!,caption={Code in C\# presenting the process of invoking the call to the Publisher API to receive single message.}, label={lst:restclientbysinglesnippet}]
{
   var response = await _httpClient.GetAsync($"http://host.docker.internal:8080/api/publisher/RESTDataProvider/GetById/{id}");

    if (response.IsSuccessStatusCode)
        {
             var data = await response.Content.ReadAsStringAsync();
                return Ok(data);
        }
        else
        {
            return StatusCode((int)response.StatusCode);
        }
\end{lstlisting}

\begin{lstlisting} [caption={Code excerpt presenting the process of consuming all messages in one request from RabbitMQ message broker}, label={lst:restclientpublishersnippet}]
    [HttpGet("GetById/{id}")]
    public IActionResult GetById(int id)
    {
        try
        {
            var message = sqLiteRepo.GetJoystickById(id);
                return Ok(message);
        }
        catch
        {
            throw;
        }
    }
\end{lstlisting}
\paragraph{Retrieving all the messages in a single request}\mbox{} \\
The process of consuming a single message from a producer application using the REST communication pattern shown in code ~\ref{lst:restclientallsnippet}.
An asynchronous HTTP GET request is sent to an external Producer API endpoint to retrieve all the data.
Code ~\ref{{lst:restproducerallsnippet}} presents how the Producer API catches the request and responses with taken messages from the database. 
Client having received  the response, reads the content of the response as a string and if the response is successful, the retrieved data is returned to the requestor. 

\begin{lstlisting} [float=h!,caption={Code in C\# presenting the process of invoking the call to the Publisher API to receive list of all messages.}, label={lst:restclientallsnippet}]
{
   var response = await _httpClient.GetAsync($"http://host.docker.internal:8080/api/publisher/RESTDataProvider/GetById/{id}");

    if (response.IsSuccessStatusCode)
        {
             var data = await response.Content.ReadAsStringAsync();
                return Ok(data);
        }
        else
        {
            return StatusCode((int)response.StatusCode);
        }
\end{lstlisting}

\begin{lstlisting} [caption={Code in C\# presenting the process of getting all messages from database.}, label={lst:restproducerallsnippet}]
    [HttpGet("GetById/{id}")]
    public IActionResult GetById(int id)
    {
        try
        {
            var message = sqLiteRepo.GetJoystickById(id);
                return Ok(message);
        }
        catch
        {
            throw;
        }
    }
\end{lstlisting}
\subsubsection{Azure Service Bus Queue Client API}
\label{subsec:AzureServiceBusFirst}
\paragraph{Retrieving individual messages in a single request}\mbox{} \\
\label{par:retriveindividualmessage}
The connection with Azure Service Bus is made using Nuget package \texttt{Azure.Messaging.ServiceBus} which provides and method to initialized and object of \texttt{IMessageReceiver} using a connection string and queue name. This instance facilitates the asynchronous reception of messages from a designated queue. The connection is established through the AzureServiceBusConnectionFactory service, maintaining a singleton lifecycle within the application. This design ensures that, the connection is established once per application life time. It prevents the scenario of the necessity of triggering a new connection  to AZURE SERVICE BUS by each request. This optimalization helps enhance processing efficiency, by avoiding unncessary slowdowns. The incoming message is stored as an object of type Message. The CompleteAsync method is invoked to indicate the message successful handling.The content  of the processed message’s body, having been transformed from its binary representation  to a UTF-8 encoded string and returned in the response of request  ~\ref{lst:restproducerallsnippet}.
 
\begin{lstlisting} [float=h!,caption={Code in C\# presenting the process of getting all messages from database.}, label={lst:restproducerallsnippet}]
IMessageReceiver messageReceiver = azureServiceBusConnectionFactory.messageReciver;
Message message = await messageReceiver.ReceiveAsync();
await messageReceiver.CompleteAsync(message.SystemProperties.LockToken);
return Ok(Encoding.UTF8.GetString(message.Body));
\end{lstlisting}
\paragraph{Retrieving all messages in a single request}\mbox{} \\

The connection with azure service bus is established in the same way as in the above section  ~\ref{par:retriveindividualmessage}. Function  ~\ref{lst:azureserviceallsnippet} establishes a continuous loop for receiving and processing messages from an Azure Service Bus queue in batches. It utilizes a predefined batchSize to retrieve groups of messages, then decodes from their binary format to UTF-8 encoded strings and stores their contents in a list. The loop iterates until there are no more messages to retrieve from the queue, effectively managing the ongoing message retrieval and processing operation.To ensure the integrity of the processing operation, the code utilizes the CompleteAsync method to signal the successful handling of each message, preventing duplicates and ensuring a  proper management of the message queue.
\begin{lstlisting} [float=h!,caption={Code in C\# presenting the process of getting all messages from database.}, label={lst:azureserviceallsnippet}]
int batchSize = 256;
List<string> messagesResult = new List<string>();
    while (true)
    {
        MessageReceiver messageReceiver = azureServiceBusConnectionFactory.messageReciver;
        var messages = await azureServiceBusConnectionFactory.messageReciver.ReceiveAsync(batchSize);

        if (messages.Count == 0)
            {
                break;
            }

        foreach (var message in messages)
            {
             var messageBody = Encoding.UTF8.GetString(message.Body);
            messagesResult.Add(messageBody);
            await azureServiceBusConnectionFactory.messageReciver.CompleteAsync(message.SystemProperties.LockToken);
            }
    }
\end{lstlisting}
\subsection{Experiments Results}
\subsubsection{Publisher}

\begin{itemize}
	\item \textbf{Performance}\\
	      The following table \ref{tab:exp1publishermetrics} presents the results collected while sending messages to data stores, taking into consideration  the communication patterns.In order to get more insides from the experiment, the data was collected during the process of transferring 300,000 data.
	      
	      \begin{table}[h]
	      	\centering
	      	\caption{Performance metrics for publishing 300 000 messages to data stores}
	      	\label{tab:exp1publishermetrics}
	      	\begin{adjustbox}{width=0.8\textwidth}
	      		\begin{tabular}{|c|c|c|c|c|}
	      			\hline
	      			\textbf{Technology}     & \textbf{Response time (s)} & \textbf{Error rate} & \textbf{Throughput (RPS)} \\
	      			\hline
	      			Rest                    & 6.015                      & 0                   & 0.166                     \\
	      			\hline
	      			RabbitMQ Queue Direct   & 10.993                     & 0                   & 0.091                     \\
	      			\hline
	      			Azure Service Bus Queue & 244.161                    & 0                   & 0.004                     \\
	      			\hline
	      		\end{tabular}
	      	\end{adjustbox}
	      	
	      \end{table}
	      The process of saving  data to the database for the REST Communication pattern, demonstrates the lowest latency and a  response time, with values of 6.015 seconds. Sending the data to the message brokers, for both Azure Service Bus Queue and RabbitMQ Queue Direct exhibit notably higher latency and response time, amounting to  244.161 seconds and 10.993 seconds, respectively.
	      The throughput, measured in requests per second, presents an interesting perspective. REST communication boasts a throughput of 0.34483 requests per unit time, which is notably higher than the message broker technologies. Azure Service Bus Queue and RabbitMQ Queue Direct exhibit lower throughput values of 0.00421 and 0.09515 requests per unit time, respectively.
	      RabbitMQ proves significantly faster, exhibiting 95.604\% higher throughput when compared to data transmission in the Azure Service Bus queue.
	      This difference could  result  from many restrictions related to data transfer that had to be taken into account during the implementation a solution for Azure Service Bus \ref{asbPublisher}. All conditional statements had to be executed in each iteration for 300000 objects, which could remarkably slow down this process remarkably. The delay also is related to the network latency described under \ref{networklatency}.
	       
	      All three technologies showcase a 0\% error rate, indicating robust and successful data transmission in each case. However the experiment proves that about 1000 messages was lost in the rabbitMQ queue. 
	      
	      For the selected scenario, the least efficient publishing messages to Azure Service Bus. However due to the fact that the data can be received by the client, also after a long time, the  following scenario, publishing process may not slow down the system. 
	\item \textbf{Implementation complexity}\\
	      The most significant challenges encountered during the implementation of publishing messages to Azure Service Bus. Creating an optimal approach for message publication wasn't straightforward and required multiple rounds of testing. Azure Service Bus offers a numerous packages versions and similar methods within .NET community. Additionally,  the configuration of queues necessitated a deep understanding of configuration options. In contrast, RabbitMQ publishing process is straightforward and it does not requiremessages to be batched for efficient handling. It's important to note that Azure Service Bus provides extensive features but can be more complex to use compared to RabbitMQ. On the other hand, while RabbitMQ is simpler to work with, it might not offer the same diversity of configuration options and message ordering management.
	      Saving messages into database,  entails the prior creatio of the database instance, repository, and tables that are suitable for storing the Joystick object. In other hand, the messages brokers (Azure Service Bus and RabbitMQ) stores object as a byte representation, allowing all objects converted to bytes to be accepted. This reduces implementation complexity when compared to using databases alongside message brokers.
\end{itemize}

\subsubsection{Consumers}

\paragraph{1 object per requests, 300 000 requests}
\label{1request per}
\subparagraph{Rest}\mbox{} \\
\label{RestExp1}
The entire experiment took 5 hours 30 minutes, involving the execution of 300,000 requests. The average response time for the experiment is 0.0064 seconds. Despite this efficiency, the maximal recorded response time reached 2.185 seconds, significantly exceeding the average response time by 34 times. Nevertheless the 99 \% of request have response time lower than 0.032 seconds.The system demonstrates an average  throughput of 15.120 requests per second. The average standard deviation stood around 0.299 second, highlights a consistent performance, though the disparity between minimum and maximum response times is substantial \ref{tab:restclientmetrics}.  
Upon scrutinizing the connection graph \ref{connectTimeRest1} and juxtaposing it with response time graph \ref{responseTimeRest1},  it can be deducted that the most of the highest response time values are strictly associated with the problem to establish TCP connection with Client API, more than the over-helmets  on the Producer side. However around  2 hours and 50 minutes of the experiment, there occurred the delay on response, which was not invoked by the connection problem. Drawing from the latency graph \ref{latencyRest1}, difference between its values and values presented response time graph \ref{responseTimeRest1} is minor, suggesting that the full response retrieval remained relatively unchallenging. 
Upon scrutinizing the connection graph \ref{connectTimeRest1} it can be deduced that there occures some problems with estabilishing the connection to the REST Client. The maximal connect time was equal to 0.005s. The future analysys about the connect time connect time is covered under  \ref{connectime}.  
\begin{table}[h]
	\centering
	\caption{Performance metrics for consuming 300000 messages, single message per request from database using REST API}
	\label{tab:restclientmetrics}
	\begin{adjustbox}{width=1\textwidth}
		\begin{tabular}{|c|c|c|c|c|c|c|}
			\hline
			\textbf{Technology} & \textbf{Average Latency (s)} & \textbf{99\% Line Latency (s)} & \textbf{MIN Latency (s)} & \textbf{Max Latency (s)} & \textbf{Throughput (RPS)} & \textbf{Std. Dev.} \\
			\hline
			REST                & 0.064                        & 0.145                          & 0.032                    & 2.185                    & 15.120                    & 0.299              \\
			\hline
		\end{tabular}
	\end{adjustbox}
\end{table}
Thought the experiment no error has been encountered, affirming the robustness of the system.
% Connect Time
\begin{figure}[!ht]
	\centering
	\begin{minipage}{\textwidth}
		\begin{subfigure}{\textwidth}
			\centering
			\includegraphics[width=0.8\linewidth]{Experiments/Experiment 1/REST/connectime.png}
			\caption{Connect time diagram for an experiment on REST communication pattern, with a single consumer and 300,000 requests.}
			\label{connectTimeRest1}
		\end{subfigure}
		    
		\vspace{\floatsep}
		    
		% Latency
		\begin{subfigure}{\textwidth}
			\centering
			\includegraphics[width=0.8\linewidth]{Experiments/Experiment 1/REST/responselatencies.png}
			\caption{Response latency diagram for an experiment on REST communication pattern, with a single consumer and 300,000 requests.}
			\label{latencyRest1}
		\end{subfigure}
		    
		\vspace{\floatsep}
		    
		% Response Time
		\begin{subfigure}{\textwidth}
			\centering
			\includegraphics[width=0.8\linewidth]{Experiments/Experiment 1/REST/responsetime.png}
			\caption{Response time diagram for an experiment on REST communication pattern, with 5 connected clients and 50,000 requests.}
			\label{responseTimeRest1}
		\end{subfigure}
		    
		\caption{Comparison of different aspects of experiments with REST communication pattern.}
	\end{minipage}
\end{figure}



\subparagraph{RabbitMQ}\mbox{} \\
\label{RabbitMQExp1}
The entire experiment 31 minutes, involving the execution of 300,000 requests. The average response time for the experiment is 0.006 seconds. Despite this efficiency, the maximal recorded response time reached 0.790 seconds, significantly exceeding the average response time by 72 times. However 99\% request took less than 0.013 to be completed.  The system demonstrates an average  throughput of 173.672 requests per second. The average standard deviation stood around 0.03 second, highlighting a consistent performance and the stability of the system, though the disparity between minimum and maximum response times is substantial \ref{tab:rabbitmqdirectsinglemetrics}.  
The latency graph \ref{latencyRabbitMQ1} and  response time graph \ref{responseTimeRabbitMQ1} indicates that there is enormous difference between the time when server send the first chunk of response and fully retriving the message body. For most of the cases the response time is two times longer than latency time.
The difference in reference to other communication patterns is analysed under \ref{latencyvsresponse} paragraph. 
\ \ref{connectTimeRabbitMQ} diagram present the time which took JMeter to establish connection with RabbitMQ Client. Beside the fist request, all of the results are about 0 s. The result indicates that the process of retrieving messages is lightweight for the system.  The difference between first request and next equals 0.417 seconds which indicates the time which is required to create connection with RabbitMQ message broker.
The \ref{latencyRabbitMQ1} presents the latency thought the whole experiment. This graph is very important because it eliminates the time, which took Jmeter to fully retrive the response, however it focuses on the time, which took RabbitMQ to get the message from broker. It can be deduced that the system found a few system overwhelms during the experiment execution. In reference to the \ref{connectTimeRabbitMQ}, this can be deduced that the delay does not rely on the problem with TCP connection establish, which is near to zero. The problem probably is related to the connection to RabbitMQ message broker instance. The first request, which involves the creation of connection to the RabbitMQ, results in the biggest anomaly and equals about 29 seconds. 

\begin{figure}[!ht]
	\begin{minipage}{\textwidth}
		\centering
		\begin{subfigure}{\textwidth}
			\centering
			\includegraphics[width=0.9\linewidth]{Experiments/Experiment 1/RABBITMQ/connectovertime.png}
			\caption{Connect time diagram for an experiment on RabbitMQ communication pattern, with a single consumer and 300,000 requests.}
			\label{connectTimeRabbitMQ}
		\end{subfigure}
		    
		\vspace{\floatsep} % Dodaj odstęp między subfigure
		    
		\begin{subfigure}{\textwidth}
			\centering
			\includegraphics[width=0.9\linewidth]{Experiments/Experiment 1/RABBITMQ/latencyovertime.png}
			\caption{Response latency diagram for an experiment on RabbitMQ communication pattern, with a single consumer and 300,000 requests.}
			\label{latencyRabbitMQ1}
		\end{subfigure}
		    
		\vspace{\floatsep} % Dodaj odstęp między subfigure
		    
		\begin{subfigure}{\textwidth}
			\centering
			\includegraphics[width=0.9\linewidth]{Experiments/Experiment 1/RABBITMQ/responsetimesobertime.png}
			\caption{Response time diagram for an experiment on RabbitMQ communication pattern, with 5 connected clients and 50,000 requests.}
			\label{responseTimeRabbitMQ1}
		\end{subfigure}
		    
		\caption{Comparison of different aspects of experiments with RabbitMQ communication pattern.}
	\end{minipage}
\end{figure}



%Metrics Table RabbitMQ
\begin{table}[h]
	\centering
	\caption{Performance metrics for consuming 300000 messages, single message per request from RabbitMQ Direct exchange.}
	\label{tab:rabbitmqdirectsinglemetrics}
	\begin{adjustbox}{width=1\textwidth}
		\begin{tabular}{|c|c|c|c|c|c|c|c|}
			\hline
			\textbf{Technology} & \textbf{Average Time (s)} & \textbf{99\% Line Time (s)} & \textbf{MIN Time (s)} & \textbf{Max Time (s)} & \textbf{Throughput (RPS)} & \textbf{Std. Dev.} \\
			\hline
			RabbitMQ            & 0.005                     & 0.013                       & 0.000                 & 0.790                 & 173.672                   & 0.00339            \\
			\hline
		\end{tabular}
	\end{adjustbox}
\end{table}

\subparagraph{Azure Service Bus} \mbox{}\\
\label{asb1}

\paragraph{300 000 objects per request, single request }\mbox{}\\
The ~\ref{tab:300000objc} presents the results of getting the list of 300 000 messages within single request. The performance metrics table (\ref{tab:300000objc}) demonstrates a stark contrast in performance among the communication patterns. 

The response times paint a revealing picture: REST records a response time of 23.73 seconds, RabbitMQ exhibits a response time of 1 hour 29 minutes and 9 seconds, while Azure Service Bus presents the longest response time at 16 hours 39 minutes and 4 seconds. Meanwhile, the throughput values stand at 0.002, 0.42141, and 0.00002 for REST, RabbitMQ, and Azure Service Bus respectively.The disparties in communication patterns are enormous. Azure Service Bus in retrieving all 300,000 messages, Azure Service Bus lags in efficiency by a factor of 2,526.13 compared to REST and by a factor of 130.29 compared to RabbitMQ.  Furthermore,  RabbitMQ is 19.39 times slower than REST. Such a good performence, REST can gain from the fact that the data either RabbitMQ and Azure Service Bus, does not implement the functionality to get all the data in one request, like REST. The messages are got separately for RabbitMQ and in batches for Azure Service Bus. This distinction underscores how REST's performance benefits from its unique approach to data retrieval, giving it a competitive edge in scenarios like the one under examination.
Compared to the test case involving a single request per action (referred to as \nameref{1request per}), a notable contrast emerges in terms of latency and response time. This discrepancy arises primarily due to the body size contained within the response. Remarkably, this disparity exhibits a consistent pattern across all forms of communication.
\begin{table}[h]
	\centering
	\caption{Performance metrics for RabbitMQ Direct All}
	\label{tab:300000objc}
	\begin{adjustbox}{width=0.8\textwidth}
		\begin{tabular}{|c|c|c|c|}
			\hline
			\textbf{Pattern}  & \textbf{Response Time (s)} & \textbf{Latency (s)} &\textbf{Throughput (RPS)} \\
			\hline
			Rest              & 2.373   &  1.847                & 0.421                   \\
			\hline
			RabbitMQ  & 460.054 & 459.310  & 0.002  \\
			\hline
			Azure Service Bus &  59944.948   &59944.656  & 0.00002 \\
			\hline
		\end{tabular}
	\end{adjustbox}
\end{table}



\section{Second experiment}
\label{Experiment2}
"The client application wants to use the data already processed by the producer application. The data created by the producer application is placed in various resources, depending on the communication technology (REST, RabbitMQ, Azure Service Bus) that microservices use. The producer application does not need to get information about whether the data has been received by the client. The client receives the data when he needs it, so that he does not store it in his system unnecessarily. The data is consumed by 5 clients simultaneously ."

\subsection{Key values of the scenario}
\begin{enumerate}[label=\arabic*.]
	\item \textbf{Asynchronous:} The communication between the producer application and the client application is asynchronous. The client does not wait for the data to be immediately available upon request but retrieves it at a later time.
	          
	\item \textbf{Decentralized Data Placement:} The data produced by the producer application is placed in various resources, depending on the communication technology (REST, RabbitMQ, Azure Service Bus) that microservices use. It implies a decentralized approach to data storage and placement.
	          
	\item \textbf{Unidirectional Communication:} The data flows from the producer to the consumer (client) in a unidirectional manner for Azure Service bus and RabbitMQ. The producer does not need to know whether the data has been received by the client.
	          
	\item \textbf{On-Demand Data Consumption:} The client receives the data when it needs it, ensuring that it does not store unnecessary data locally. This efficient data consumption approach minimizes storage overhead on the client's side.
	          
	\item \textbf{Parallelism and Scalability:} The data is concurrently consumed by up to 5 clients, enabling multiple clients to access and utilize the processed data. The presence of multiple consumers allows for parallel processing of data. Each consumer can independently retrieve and process data, enabling the system to handle a higher volume of requests simultaneously.
\end{enumerate}
\subsection{Experiment technical arrangement}
\subsubsection{Publisher}
\paragraph{Azure Service Bus Topic publisher}\mbox{}\\
In the context of handling multiple consumers, the Azure Service Bus topics were employed to facilitate the required functionality. The underlying process described under the \ref{sec:firstexperimenttechnicalarrangement} section, responsible for sending messages to an Azure Service Bus queue remains largely analogous, however, in this case the \textbf{ServiceBusClient} object is instantiated with respect to the designated topic name.
The code responsible for publishing messages to Topic in the Azure Service Bus  ~\ref{lst:azurepublishertopicsnippet}.
\begin{lstlisting} [float=h!,caption={Code in C\# presenting the process of sending messages to the specified topic on the Azure Service Bus.}, label={lst:azurepublishertopicsnippet}]
List<ServiceBusMessage> serviceBusMessages = new List<ServiceBusMessage>();
var serviceBusMessageBatch = await sender.CreateMessageBatchAsync();
for (int i = 0; i < message.Count; i++)
{
    var messageBytes = Encoding.UTF8.GetBytes(String.Join(",", message[i].time, message[i].axis_1, message[i].axis_2, message[i].button_1,        message[i].button_2, message[i].id.ToString()));

    if (!serviceBusMessageBatch.TryAddMessage(new ServiceBusMessage(messageBytes)))
    {
        await sender.SendMessagesAsync(serviceBusMessageBatch);
        serviceBusMessageBatch.Dispose();
        serviceBusMessageBatch = await sender.CreateMessageBatchAsync();
        serviceBusMessageBatch.TryAddMessage(new ServiceBusMessage(messageBytes));
    }
}

await sender.SendMessagesAsync(serviceBusMessageBatch);
\end{lstlisting}
\paragraph{RabbitMQ fanout exchange client publisher}\mbox{}\\
The scenario for publishing messages to the fanout exchange on the RabbitMQ message broker is analogous to publishing messages to direct exchange described under \ref{sec:firstexperimenttechnicalarrangement}. 
Although the functionality has to be extended by declaring each queue for each consumer and their bing to fanout exchange. 

The process is show in code ~\ref{lst:rabbitmqfanoutsnippet}. Messages are published to fanout exchange and then the copy of messages is placed on each queue, which has declared binding to it, by RabbitMQ. 

\begin{lstlisting} [float=h!,caption={Code in C\# presenting the process of sending messages to the fanout exchange on RabbitMQ message broker.}, label={lst:rabbitmqfanoutsnippet}]
channel.ExchangeDeclare(exchange: "my-fanout-exchange", type: ExchangeType.Fanout);

channel.QueueDeclare("consumer1", durable: false, autoDelete: false, exclusive: false);
channel.QueueDeclare("consumer2", durable: false, autoDelete: false, exclusive: false);
channel.QueueDeclare("consumer3", durable: false, autoDelete: false, exclusive: false);
channel.QueueDeclare("consumer4", durable: false, autoDelete: false, exclusive: false);
channel.QueueDeclare("consumer5", durable: false, autoDelete: false, exclusive: false);

channel.QueueBind("consumer1", "my-fanout-exchange", "");
channel.QueueBind("consumer2", "my-fanout-exchange", "");
channel.QueueBind("consumer3", "my-fanout-exchange", "");
channel.QueueBind("consumer4", "my-fanout-exchange", "");
channel.QueueBind("consumer5", "my-fanout-exchange", "");

foreach (Joystick joystick in message)
{
    var id = Guid.NewGuid();
    channel.BasicPublish(exchange: "my-fanout-exchange",
                         routingKey: "",
                         basicProperties: null,
                         body: Encoding.UTF8.GetBytes(String.Join(",", joystick.time, joystick.axis_1, joystick.axis_2, joystick.button_1, joystick.button_2, Guid.NewGuid().ToString())));
}

return Task.CompletedTask;
\end{lstlisting}
    
\paragraph{REST}\mbox{} \\
\label{RESTExperiment1}
In the scenario involving the storage of data generated by a publisher API within the context of the Representational State Transfer communication pattern, catering to multiple consumers, the mechanism described in \nameref{sec:firstexperimenttechnicalarrangement}—originally designed for a singular consumer—remained unaltered, necessitating no supplementary modifications.
\subsubsection{Azure Service Bus Client API}
In both scenarios: 'retrieving individual messages in a single' and 'retrieving all messages in a single request' the approach to message consumption bears resemblance to the one described under \nameref{subsec:AzureServiceBusFirst}. Nevertheless, in subsequent scenario the ServiceBusReceiver object is instantiated using the topic name and subscription, taken from environment variables specified in the docker compose file.
\subsubsection{RabbitMQ Fanout Exchange Client API}
In both scenarios : 'retrieving individual messages in a single' and 'retrieving all messages in a single request' the implementation of the approach to message consumption bears resemblance to the one outlined in \nameref{subsec:rabbitMQFirst}. Nevertheless, in subsequent scenario the name of queue is obtained from environment variables specified in the docker compose file.
\subsubsection{REST Client API}
For both scenarios: 'retrieving individual messages in a single' and 'retrieving all messages in a single request' the implementation of functionality  remains consistent to the functionality discussed in the initial experiment, elaborated in the section \nameref{subsec:restfirst}.

\subsection{Experiments Results}
\subsubsection{Publisher}
\begin{table}[h]
	\centering
	\caption{Performance metrics for publishing 300 000 messages to data stores}
	\label{tab:exp1publishermetrics}
	\begin{adjustbox}{width=0.8\textwidth}
		\begin{tabular}{|c|c|c|c|c|}
			\hline
			\textbf{Technology}     & \textbf{Response time (s)} & \textbf{Error rate} & \textbf{Throughput (RPS)} \\
			\hline
			Rest                    & 6.015                      & 0                   & 0.166                     \\
			\hline
			Azure Service Bus Topic & 1386.359                   & 0                   & 0.0007                    \\
			\hline
			RabbitMQ Queue Fanout   & 16.291                     & 0                   & 0.0614                    \\
			\hline
		\end{tabular}
	\end{adjustbox}
\end{table}
\subsubsection{REST}
\label{exp2rest}
\begin{table}[h]
	\centering
	\caption{Performance metrics for consuming 50 000 messages, single message per request using REST, with 5 instances of clients requesting data simultaneously }
	\label{tab:exp1publishermetrics}
	\begin{adjustbox}{width=1\textwidth}
		\begin{tabular}{|c|c|c|c|c|c|c|}
			\hline
			\textbf{Label} & \textbf{Average  (s)} & \textbf{MIN  (s)} & \textbf{MAX  (s)} & \textbf{99\% Line  (s)} & \textbf{Throughput (RPS)} & \textbf{Std. Dev.} \\
			\hline
			REST CLIENT 1  & 0.266                 & 0.046             & 6.352             & 1.304                   & 3.273                     & 0.254              \\
			REST CLIENT 2  & 0.265                 & 0.048             & 10.506            & 1.314                   & 3.268                     & 0.251              \\
			REST CLIENT 3  & 0.266                 & 0.049             & 7.589             & 1.308                   & 3.270                     & 0.252              \\
			REST CLIENT 4  & 0.265                 & 0.048             & 9.060             & 1.312                   & 3.275                     & 0.251              \\
			REST CLIENT 5  & 0.266                 & 0.047             & 10.197            & 1.308                   & 3.278                     & 0.253              \\
			\hline
			TOTAL          & 0.265                 & 46                & 10.506            & 1.310                   & 16.34                     & 0.252              \\
			\hline
		\end{tabular}
	\end{adjustbox}
\end{table}

The entire experiment took 4h and 15 minutes. The average latency for the experiment is 0.265 seconds. Each of the client applications achieved a throughput of approximately 3.3 requests per second .The average standard deviation stood around 0.252 seconds, which in relatively high in compared the average latency. This discrepancy  could suggest potential system instability. The the longest time recorded for a request to completed was 10.506 seconds. Additionally, the 99th percentile latency value indicates that there was 1\% of request that took more than 1.2 second to be processed. The difference in processing time, in comparison to the minimal latency value is noteworthy.
As it is presented on the \ref{latencyrest2} figure, the experiment encountered a period of system overload between 1-hour and 15-minute and 2-hour and 20-minute mark of experiment. Notably, at the 1 hour and 41-minute mark the highest anomaly was observed. This anomaly had a widespread impact, influencing all client applications.

The visual representation in\ref{conenctimerest2} illustrates the time taken by JMeter to establish a connection with REST Client instance. The most significant anomaly occured for Rest Client 1 instance at 1-hour and 16-minute mark where the connect time reached approximately 0.245 seconds. Future analyses about the connect time values is covered under \nameref{connectime} section.

Upon analyzing the results in conjunction with those presented the figure \ref{responstimerest2}, it can be deduced that the  exhibiting higher response times are likely experiencing delays on the client side. However the highest response time is closely related to the problems on the publisher application'a side, as the connect time to all client remains consistently low at this time. The issue consequently affects all  request, regardless of the specific client application that initiated them.

In reference to the first experiment \nameref{RestExp1}, the process of consuming messages by 5 clients simultaneously has 5 times slower results. The response time average equals 64 milliseconds and 265 milliseconds respectively for \nameref{Experiment1} and \nameref{Experiment2}. The 99th percentile latency value for the second experiment signify that 99\% response times is lower than approximately  1.3 seconds for each instance, however for the fist experiment following value equals 0.145 s which makes result 10 times greater. The differences indicates that the Publisher API does not automatically scale and it distributes the same set of resources among all consumers, thereby causing a slowdown proportional to the number of consumers. This solution requires future analyses and optimalization in order to serve resources between multiple clients. 
\begin{figure}[!ht]
	\centering
	\begin{minipage}{\textwidth}
		% Connect Time
		\begin{subfigure}{1\textwidth}
			\centering
			\includegraphics[width=0.8\linewidth]{Experiments/experiment 2/rest/connect times over time.png}
			\caption{Connect time diagram for an experiment on REST communication pattern, with 5 connected clients and 50,000 requests.}
			\label{conenctimerest2}
		\end{subfigure}
		        
		\vspace{\floatsep}
		        
		% Latency
		\begin{subfigure}{1\textwidth}
			\centering
			\includegraphics[width=0.8\linewidth]{Experiments/experiment 2/rest/response latencies over time.png}
			\caption{Response latency diagram for an experiment on REST communication pattern, with 5 connected clients and 50,000 requests.}
			\label{latencyrest2}
		\end{subfigure}
		        
		\vspace{\floatsep}
		        
		% Response Time
		\begin{subfigure}{1\textwidth}
			\centering
			\includegraphics[width=0.8\linewidth]{Experiments/experiment 2/rest/response time over time.png}
			\caption{Response time diagram for an experiment on REST communication pattern, with 5 connected clients and 50,000 requests.}
			\label{responstimerest2}
		\end{subfigure}
		        
		\caption{Comparison of different aspects of experiments with REST communication pattern in Experiment 2.}
	\end{minipage}
\end{figure}

\subsubsection{RabbitMQ}

The entire experiment took 10m . The average latency for the experiment is 0.009 seconds. The throughput for most of the Consumers is similar and equals about 82 request per second.  The average standard deviation stood around 0.006 seconds, which indicates very high system consistency.  The the longest time recorded for a request to completed is 0.587 seconds which is approximately 62 times more than the average. However, the 99th percentile latency value indicates that there the 99\% of request that took less than 0.033 second to be processed, which indicates that the max value is non common anomaly. 
As it is presented on the \ref{latencyrest2} figure, the experiment encountered a period of system overload about 5th-minute mark of experiment, where the anomally occured. 

The diagram \ref{connectTimeRabbitMQ} presents the visual depiction of latency, showcasing the time JMeter requires to establish connections with instances of RabbitMQ Consumers. Regarding the findings from \ref{RabbitMQExp1}, the highest connection time is observed during the initial request. This observation is in line with the latency diagram \ref{latencyRabbitMQ2}, where the latency for the first request notably surpasses that of subsequent requests by approximately 0.120 seconds. Consequently, across all instances of the application, the connection initialization process with RabbitMQ consistently involves this duration.
Comparing the \ref{responsetimeRabbitMQ2} and \ref{latencyRabbitMQ2}, it can be deducted that the process of retrieving full message returned by client instances is smooth. 

In reference to the first experiment \ref{RabbitMQExp1}
the process of consuming messages by 5 clients simultaneously has slown down the process about twice time. The throuhput for the first experiment equals 173.673 request per second and for the second experiment equals about 82 request per second for single instance. Respectively the averange response time equals 0.005 of second for the first experiment and 0.009 of second for second experiment.
\begin{table}[h]
	\centering
	\caption{Performance metrics for publishing 250,000 messages to data stores}
	\label{tab:exp1publishermetrics}
	\begin{adjustbox}{width=1\textwidth}
		\begin{tabular}{|c|c|c|c|c|c|c|c|}
			\hline
			\textbf{Label}      & \textbf{Average (s)} & \textbf{99\% Line (s)} & \textbf{Min (s)} & \textbf{Max (s)} & \textbf{Throughput (RPS)} & \textbf{Std. Dev.} \\
			\hline
			RabbitMQ Consumer 1 & 0.009                & 0.033                  & 0.003            & 0.586            & 82.556                    & 0.00671            \\
			RabbitMQ Consumer 2 & 0.009                & 0.033                  & 0.003            & 0.571            & 83.266                    & 0.00647            \\
			RabbitMQ Consumer 3 & 0.009                & 0.033                  & 0.003            & 0.440            & 82.211                    & 0.00649            \\
			RabbitMQ Consumer 4 & 0.009                & 0.033                  & 0.003            & 0.587            & 82.859                    & 0.00665            \\
			RabbitMQ Consumer 5 & 0.009                & 0.033                  & 0.003            & 0.439            & 82.393                    & 0.00669            \\
			\hline
			TOTAL               & 0.009                & 0.033                  & 0.003            & 0.587            & 411.056                   & 0.0066             \\
			\hline
		\end{tabular}
	\end{adjustbox}
\end{table}


\begin{figure}[!ht]
	\centering
	\begin{minipage}{\textwidth}
		% Connect Time
		\begin{subfigure}{1\textwidth}
			\centering
			\includegraphics[width=0.8\linewidth]{Experiments/experiment 2/RabbitMQ/connect over time.png}
			\caption{Connect time diagram for an experiment on RabbitMQ communication pattern, with 5 connected clients and 50,000 requests.}
			\label{conenctimeRabbitMQ2}
		\end{subfigure}
		        
		\vspace{\floatsep}
		        
		% Latency
		\begin{subfigure}{1\textwidth}
			\centering
			\includegraphics[width=0.8\linewidth]{Experiments/experiment 2/RabbitMQ/response latencies over time.png}
			\caption{Response latency diagram for an experiment on RabbitMQ communication pattern, with 5 connected clients and 50,000 requests.}
			\label{latencyRabbitMQ2}
		\end{subfigure}
		        
		\vspace{\floatsep}
		        
		% Response Time
		\begin{subfigure}{1\textwidth}
			\centering
			\includegraphics[width=0.8\linewidth]{Experiments/experiment 2/RabbitMQ/responses overtime.png}
			\caption{Response time diagram for an experiment on RabbitMQ communication pattern, with 5 connected clients and 50,000 requests.}
			\label{responsetimeRabbitMQ2}
		\end{subfigure}
		        
		\caption{Comparison of different aspects of experiments with RabbitMQ communication pattern in Experiment 2.}
	\end{minipage}
\end{figure}

\subsubsection{Azure Service Bus}
The entire experiment took 4h and 10 minutes. The average latency for the experiment is 0.327 seconds.The throughput between client API differ and values reached from the range from 2.974 to 3.027 requests per second .The average standard deviation stood around 0.102 seconds, which in comparison to other results is low. The the longest time recorded for a request to completed was 3.986 seconds which is aproximately 10 times more than the average. However, the 99th percentile latency value indicates that there the 99\% of request that took less than 0.382 second to be processed, which indicates that the values around the max value are just non common occurred anomaly.
As it is presented on the \ref{latencyrest2} figure, the experiment encountered a period of system overload between 1-hour and 15-minute and 2-hour and 20-minute mark of experiment. Notably, at the 1 hour and 41-minute mark the highest anomaly was observed. This anomaly had a widespread impact, influencing all client applications.

The visual representation of latency time \ref{conenctimeasb2} illustrates the time taken by JMeter to establish a connection with Azure Service Bus Topic Clients instances. The the connection time values does not exceed the 0.004 s and after the analys in conjunction with values presented on the latency diagram \ref{latencyasb2} it can be deduced that delays in the connect time does no have significant impact on the latency.
The latency time diagram \ref{latencyasb2} indicates that the first all request are distinguished by higher latency consequently for all instances of the application. It means that the initialization of connection with Azure Service Bus could significantly slow down the process of consuming messages. There occurs multiple anomalies however the lower are for the last one - Azure Service Bus Topic Client 5. It can be deducted that this particular  instance has finished experiment few minutes particular of other. Additionally there anomalies among the instances of Azure Service Bus Client Topic API'S seem to lack a discernible pattern. Each anomaly manifests independently for a single instance and does not exert any influence on the others. This could indicates the temporary unstability of azure resources. 


\begin{figure}[!ht]
	\centering
	\begin{minipage}{\textwidth}
		% Connect Time
		\begin{subfigure}{1\textwidth}
			\centering
			\includegraphics[width=0.8\linewidth]{Experiments/experiment 2/asb/connect time over time.png}
			\caption{Connect time diagram for an experiment on Azure Service Bus communication pattern, with 5 connected clients and 50,000 requests.}
			\label{conenctimeasb2}
		\end{subfigure}
		        
		\vspace{\floatsep}
		        
		% Latency
		\begin{subfigure}{1\textwidth}
			\centering
			\includegraphics[width=0.8\linewidth]{Experiments/experiment 2/asb/responses latencies over time.png}
			\caption{Response latency diagram for an experiment on Azure Service Bus communication pattern, with 5 connected clients and 50,000 requests.}
			\label{latencyasb2}
		\end{subfigure}
		        
		\vspace{\floatsep}
		        
		% Response Time
		\begin{subfigure}{1\textwidth}
			\centering
			\includegraphics[width=0.8\linewidth]{Experiments/experiment 2/asb/response times overtimes.png}
			\caption{Response time diagram for an experiment on Azure Service Bus communication pattern, with 5 connected clients and 50,000 requests.}
			\label{responsetimeasb2}
		\end{subfigure}
		        
		\caption{Comparison of different aspects of experiments with Azure Service Bus communication pattern in Experiment 2.}
	\end{minipage}
\end{figure}
\begin{table}[h]
	\centering
	\caption{Performance metrics for consuming 50 000 messages, single message per request from Azure Service Bus Topic, with 5 consumers connected simultaneously}
	\label{tab:exp2publishermetrics}
	\begin{adjustbox}{width=1\textwidth}
		\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
			\hline
			\textbf{Label}               & \textbf{Average (s)} & \textbf{90\% Line (s)} & \textbf{MIN (s)} & \textbf{MAX (s)} & \textbf{Throughput (RPS)} & \textbf{Std. Dev.} \\
			\hline
			Azure Service Bus Consumer 1 & 0.331                & 0.385                  & 0                & 3.986            & 2.974                     & 0.084              \\
			Azure Service Bus Consumer 2 & 0.325                & 0.377                  & 0                & 3.737            & 3.003                     & 0.130              \\
			Azure Service Bus Consumer 3 & 0.328                & 0.377                  & 0                & 3.708            & 2.996                     & 0.112              \\
			Azure Service Bus Consumer 4 & 0.325                & 0.395                  & 0                & 3.666            & 3.027                     & 0.101              \\
			Azure Service Bus Consumer 5 & 0.325                & 0.377                  & 0                & 2.965            & 3.016                     & 0.068              \\
			\hline
			TOTAL                        & 0.327                & 0.382                  & 0                & 3.986            & 14.872                    & 0.102              \\
			\hline
		\end{tabular}
	\end{adjustbox}
\end{table}


\section{Experiments results summary}
\paragraph{Network latency}\mbox{} \\
\label{networklatency}
For all of the performed experiments, results for the Azure Service Bus, a cloud broker representative, are significantly poorer. RabbitMQ message broker has deployed it's instance within the local docker container, as well as Publisher API, from where the data was taken in the REST experiment. All of Client API's are also deployed in the docker containers within the same local environment , and they share the same network with each other. It indicates that the latency between all instances is near to zero. 
Azure Service Bus is located on the server in EAST US Azure region, which has physical location in Richmond in Virginia.   The average network latency to provided data ceneter is about 180 ms. It indicates that within all the request to Azure Service Bus, the latency slows down request finalisation. The connection to Azure Service Bus  also depends on the current server availability. The possibility of getting data from it, requires the token authorization which could also slow down the process.  
To draw more precisely conclusion about the influence of the latency to external messages broker it is necessary to perform more experiment directed to that topic. Exemplary experiment is comparison of connection to the RabbitMQ message broker run on the virtual machine and the one run on the local environment. 

\paragraph{Difference between latency and response time}\mbox{} \\
\label{latencyvsresponse}
The resulting differences stem from the specific steps involved in processing data in these three cases. RabbitMQ returns message in representation of the bytes, what involves converting the entire response body into a byte array and then further converting that array into a string. This process might introduce more significant delays since it includes two conversions and memory operations. REST Client reads the Publisher's response content directly as a string, bypassing the intermediate step of converting to a byte array. and incurs smaller delays compared to the RabbitMQ approach, as it doesn't involve the added overhead of unnecessary conversions. The \ref{asb1} message body as an array of bytes and decodes  the message body into a UTF-8 encoded string, eliminating the need for an additional conversion steps. This approach is streamlined resulting in minimal delays compared to the RabbitMQ scenario.
\paragraph{Connect time}\mbox{} \\
Connect time refers to the period elapsed from the initiation of a communication request by a client application to the establishment of an active connection with a server. In the context of networking and system performance analysis, connect time is a crucial metric that evaluates the efficiency and responsiveness of communication between clients and servers.
\label{connectime}
The connect time refers to the duration from when a client initiates a request to when the server accepts the request and establishes a connection. If this time is significantly longer than usual, it might suggest performance issues with the system or server. It can be caused by to many amount of requests than system can process. Application can struggle to handle all of them, leading to extend the connect time. Such as scenario is called "the system overload". The connect time can also signify the problem with network or hardware issues.  In the experiment performed in following thesis, the network and and hardware was monitored and no problem has been noticed. 

In the experiment performed under the \nameref{Experiment1} and \nameref{Experiment2} sections, has been recorded multiple connect time anomalies, which are taken into consideration. 

For the experiments using REST pattern, both for of experiments, have occured numerous long connection time values. As for the experiment 1, the highest number is equal to 5 milliseconds, so for the experiment 2, the highest value is 250 ms, which can results in the serious performance issues. This indicates that in case of used selected communication pattern, there is need to invest in better computation power hardware, with option to auto-scaling in order of receiving the higher number of request, to handle them. 

For the experiment using RabbitMQ pattern, for both of experiments, the connection time is noticeable only for the initial request. It indicates that the system is less susceptible for the system overload while creating the larger number of requests simultaneously. 

For the experiment using Azure Service Bus pattern, for the second experiment the connection times has been recorded. 




\chapter{Summary}
\chapter{Limitations of this study}




\begin{appendices}
	% TODO
	\chapter{Technical documentation}
	
	
	% TODO
	\chapter{List of abbreviations and symbols}
	
	\begin{itemize}
		\item[REST] deoxyribonucleic acid
		\item[API] model--view--controller 
		\item[AMPQ] cardinality of data set
		\item[ HTTP] membership function of a fuzzy set
		\item[SOA] membership function of a fuzzy set
		\item[JAX] deoxyribonucleic acid
		\item[ LXC] model--view--controller 
		\item[TCP] cardinality of data set
		\item[ HTTP] membership function of a fuzzy set
		\item[SOA] membership function of a fuzzy set
		\item[$\mathbb{E}$] set of edges of a graph
		\item[$\mathcal{L}$] Laplace transformation
	\end{itemize}
	
	
	
	Additional files uploaded to the system include:
	\begin{itemize}
		\item source code of the application,
		\item test data,
		\item a video file showing how software or hardware developed for thesis is used,
		\item etc.
	\end{itemize}
	
	A book on game theory is \cite{basar1995dynamic}.
	
	
	
	
	
	\listoffigures
	\addcontentsline{toc}{chapter}{List of figures}
	\listoftables
	\addcontentsline{toc}{chapter}{List of tables}
	
	\newpage
	\printbibliography           % biblatex
	\addcontentsline{toc}{chapter}{Bibliography}
\end{appendices}

\end{document}


%% Finis coronat opus.



%% Finis coronat opus.

